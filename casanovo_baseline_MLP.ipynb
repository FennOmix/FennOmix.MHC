{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3acf2fac",
   "metadata": {},
   "source": [
    "**Casanovo baseline**, 主要用于练手和后期调试基础"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f52fb7",
   "metadata": {},
   "source": [
    "**Config**\n",
    "\n",
    "配置yaml，模型参数，质谱数据处理参数等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "696f9799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import warnings\n",
    "import logging\n",
    "import platform\n",
    "import shutil\n",
    "import psutil\n",
    "import torch\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Union, Dict, Optional, Callable, Tuple, ItemsView\n",
    "\n",
    "logger = logging.getLogger(\"casanovo\")\n",
    "\n",
    "\n",
    "class utils:\n",
    "    @staticmethod \n",
    "    def n_workers() -> int:\n",
    "        \"\"\"Get the number of workers to use for data loading.\"\"\"\n",
    "        if platform.system() in [\"Windows\", \"Darwin\"]:\n",
    "            logger.warning(\n",
    "                \"Dataloader multiprocessing is currently not supported on Windows \"\n",
    "                \"or MacOS; using only a single thread.\"\n",
    "            )\n",
    "            return 0\n",
    "        try:\n",
    "            n_cpu = len(psutil.Process().cpu_affinity())\n",
    "        except AttributeError:\n",
    "            n_cpu = os.cpu_count()\n",
    "        return n_cpu // torch.cuda.device_count() if torch.cuda.device_count() > 1 else n_cpu\n",
    "\n",
    "    @staticmethod\n",
    "    def split_version(version: str) -> Tuple[str, str, str]:\n",
    "        \"\"\"Split the version into major, minor, and patch.\"\"\"\n",
    "        version_regex = re.compile(r\"(\\d+)\\.(\\d+)\\.*(\\d*)\")\n",
    "        return tuple(version_regex.match(version).groups())\n",
    "\n",
    "\n",
    "class Config:\n",
    "    # Handle __file__ for notebook compatibility\n",
    "    try:\n",
    "        _default_config = Path(__file__).parent / \"config.yaml\"\n",
    "    except NameError:\n",
    "        _default_config = Path.cwd() / \"config.yaml\"\n",
    "\n",
    "    _config_deprecated = dict(\n",
    "        every_n_train_steps=\"val_check_interval\",\n",
    "        max_iters=\"cosine_schedule_period_iters\",\n",
    "    )\n",
    "\n",
    "    _config_types = dict(\n",
    "        random_seed=int,\n",
    "        n_peaks=int,\n",
    "        min_mz=float,\n",
    "        max_mz=float,\n",
    "        min_intensity=float,\n",
    "        remove_precursor_tol=float,\n",
    "        max_charge=int,\n",
    "        precursor_mass_tol=float,\n",
    "        isotope_error_range=lambda x: (int(x[0]), int(x[1])),\n",
    "        min_peptide_len=int,\n",
    "        dim_model=int,\n",
    "        n_head=int,\n",
    "        dim_feedforward=int,\n",
    "        n_layers=int,\n",
    "        dropout=float,\n",
    "        dim_intensity=int,\n",
    "        max_length=int,\n",
    "        residues=dict,\n",
    "        n_log=int,\n",
    "        tb_summarywriter=str,\n",
    "        train_label_smoothing=float,\n",
    "        warmup_iters=int,\n",
    "        cosine_schedule_period_iters=int,\n",
    "        learning_rate=float,\n",
    "        weight_decay=float,\n",
    "        train_batch_size=int,\n",
    "        predict_batch_size=int,\n",
    "        n_beams=int,\n",
    "        top_match=int,\n",
    "        max_epochs=int,\n",
    "        num_sanity_val_steps=int,\n",
    "        save_top_k=int,\n",
    "        model_save_folder_path=str,\n",
    "        val_check_interval=int,\n",
    "        calculate_precision=bool,\n",
    "        accelerator=str,\n",
    "        devices=int,\n",
    "    )\n",
    "\n",
    "    def __init__(self, config_file: Optional[str] = None):\n",
    "        self.file = str(config_file) if config_file else \"default\"\n",
    "\n",
    "        with self._default_config.open() as f:\n",
    "            self._params = yaml.safe_load(f)\n",
    "\n",
    "        self._user_config = {}\n",
    "\n",
    "        if config_file:\n",
    "            with Path(config_file).open() as f:\n",
    "                self._user_config = yaml.safe_load(f)\n",
    "\n",
    "            for old, new in self._config_deprecated.items():\n",
    "                if old in self._user_config:\n",
    "                    self._user_config[new] = self._user_config.pop(old)\n",
    "                    warnings.warn(\n",
    "                        f\"Deprecated config option '{old}' remapped to '{new}'\",\n",
    "                        DeprecationWarning,\n",
    "                    )\n",
    "\n",
    "            missing = self._params.keys() - self._user_config.keys()\n",
    "            unknown = self._user_config.keys() - self._params.keys()\n",
    "\n",
    "            if missing:\n",
    "                raise KeyError(f\"Missing config option(s): {', '.join(missing)}\")\n",
    "            if unknown:\n",
    "                raise KeyError(f\"Unrecognized config option(s): {', '.join(unknown)}\")\n",
    "\n",
    "        for key, val in self._config_types.items():\n",
    "            self.validate_param(key, val)\n",
    "\n",
    "        self._params[\"n_workers\"] = utils.n_workers()\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, config_dict: Dict[str, Union[int, float, str, bool, dict, tuple]]) -> \"CasanovoConfig\":\n",
    "        \"\"\"Initialize from a dictionary (useful in notebooks)\"\"\"\n",
    "        instance = cls(config_file=None)\n",
    "        instance._user_config = config_dict\n",
    "        for key, val in cls._config_types.items():\n",
    "            instance.validate_param(key, val)\n",
    "        instance._params[\"n_workers\"] = utils.n_workers()\n",
    "        return instance\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Union[int, float, str, bool, dict, tuple]]:\n",
    "        \"\"\"Export the current configuration as a dictionary.\"\"\"\n",
    "        return dict(self._params)\n",
    "\n",
    "    def __getitem__(self, param: str) -> Union[int, float, str, bool, Dict, Tuple]:\n",
    "        return self._params[param]\n",
    "\n",
    "    def __getattr__(self, param: str) -> Union[int, float, str, bool, Dict, Tuple]:\n",
    "        return self._params[param]\n",
    "\n",
    "    def items(self) -> ItemsView[str, Union[int, float, str, bool, Dict, Tuple]]:\n",
    "        return self._params.items()\n",
    "\n",
    "    def validate_param(self, param: str, param_type: Callable):\n",
    "        try:\n",
    "            value = self._user_config.get(param, self._params[param])\n",
    "            if param == \"residues\":\n",
    "                self._params[param] = {str(k): float(v) for k, v in value.items()}\n",
    "            elif value is not None:\n",
    "                self._params[param] = param_type(value)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Invalid value for config '{param}': {e}\")\n",
    "            raise TypeError(f\"Invalid config type for {param}: {e}\")\n",
    "\n",
    "    @classmethod\n",
    "    def copy_default(cls, output: str) -> None:\n",
    "        \"\"\"Copy the default YAML to output path.\"\"\"\n",
    "        shutil.copyfile(cls._default_config, output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c374b111",
   "metadata": {},
   "source": [
    "**Result IO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd59398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "import operator\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import natsort\n",
    "\n",
    "logger = logging.getLogger(\"casanovo\")\n",
    "class MztabWriter:\n",
    "    \"\"\"\n",
    "    Export spectrum identifications to an mzTab file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        The name of the mzTab file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename: str):\n",
    "        self.filename = filename\n",
    "        self.metadata = [\n",
    "            (\"mzTab-version\", \"1.0.0\"),\n",
    "            (\"mzTab-mode\", \"Summary\"),\n",
    "            (\"mzTab-type\", \"Identification\"),\n",
    "            (\n",
    "                \"description\",\n",
    "                f\"Casanovo identification file \"\n",
    "                f\"{os.path.splitext(os.path.basename(self.filename))[0]}\",\n",
    "            ),\n",
    "            (\"software[1]\", f\"[MS, MS:1003281, Casanovo, notebook version]\"),\n",
    "            (\n",
    "                \"psm_search_engine_score[1]\",\n",
    "                \"[MS, MS:1001143, search engine specific score for PSMs, ]\",\n",
    "            ),\n",
    "        ]\n",
    "        self._run_map = {}\n",
    "        self.psms = []\n",
    "\n",
    "    def set_metadata(self, config: Config, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Specify metadata information to write to the mzTab header.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config : Config\n",
    "            The active configuration options.\n",
    "        kwargs\n",
    "            Additional configuration options (i.e. from command-line arguments).\n",
    "        \"\"\"\n",
    "        # Derive the fixed and variable modifications from the residue alphabet.\n",
    "        known_mods = {\n",
    "            \"+57.021\": \"[UNIMOD, UNIMOD:4, Carbamidomethyl, ]\",\n",
    "            \"+15.995\": \"[UNIMOD, UNIMOD:35, Oxidation, ]\",\n",
    "            \"+0.984\": \"[UNIMOD, UNIMOD:7, Deamidated, ]\",\n",
    "            \"+42.011\": \"[UNIMOD, UNIMOD:1, Acetyl, ]\",\n",
    "            \"+43.006\": \"[UNIMOD, UNIMOD:5, Carbamyl, ]\",\n",
    "            \"-17.027\": \"[UNIMOD, UNIMOD:385, Ammonia-loss, ]\",\n",
    "        }\n",
    "        residues = collections.defaultdict(set)\n",
    "        for aa, mass in config[\"residues\"].items():\n",
    "            aa_mod = re.match(r\"([A-Z]?)([+-]?(?:[0-9]*[.])?[0-9]+)\", aa)\n",
    "            if aa_mod is None:\n",
    "                residues[aa].add(None)\n",
    "            else:\n",
    "                residues[aa_mod[1]].add(aa_mod[2])\n",
    "        fixed_mods, variable_mods = [], []\n",
    "        for aa, mods in residues.items():\n",
    "            if len(mods) > 1:\n",
    "                for mod in mods:\n",
    "                    if mod is not None:\n",
    "                        variable_mods.append((aa, mod))\n",
    "            elif None not in mods:\n",
    "                fixed_mods.append((aa, mods.pop()))\n",
    "\n",
    "        # Add all config values to the mzTab metadata section.\n",
    "        if len(fixed_mods) == 0:\n",
    "            self.metadata.append(\n",
    "                (\n",
    "                    \"fixed_mod[1]\",\n",
    "                    \"[MS, MS:1002453, No fixed modifications searched, ]\",\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            for i, (aa, mod) in enumerate(fixed_mods, 1):\n",
    "                self.metadata.append(\n",
    "                    (\n",
    "                        f\"fixed_mod[{i}]\",\n",
    "                        known_mods.get(mod, f\"[CHEMMOD, CHEMMOD:{mod}, , ]\"),\n",
    "                    )\n",
    "                )\n",
    "                self.metadata.append(\n",
    "                    (f\"fixed_mod[{i}]-site\", aa if aa else \"N-term\")\n",
    "                )\n",
    "        if len(variable_mods) == 0:\n",
    "            self.metadata.append(\n",
    "                (\n",
    "                    \"variable_mod[1]\",\n",
    "                    \"[MS, MS:1002454, No variable modifications searched,]\",\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            for i, (aa, mod) in enumerate(variable_mods, 1):\n",
    "                self.metadata.append(\n",
    "                    (\n",
    "                        f\"variable_mod[{i}]\",\n",
    "                        known_mods.get(mod, f\"[CHEMMOD, CHEMMOD:{mod}, , ]\"),\n",
    "                    )\n",
    "                )\n",
    "                self.metadata.append(\n",
    "                    (f\"variable_mod[{i}]-site\", aa if aa else \"N-term\")\n",
    "                )\n",
    "        for i, (key, value) in enumerate(kwargs.items(), 1):\n",
    "            self.metadata.append(\n",
    "                (f\"software[1]-setting[{i}]\", f\"{key} = {value}\")\n",
    "            )\n",
    "        for i, (key, value) in enumerate(config.items(), len(kwargs) + 1):\n",
    "            if key not in (\"residues\",):\n",
    "                self.metadata.append(\n",
    "                    (f\"software[1]-setting[{i}]\", f\"{key} = {value}\")\n",
    "                )\n",
    "\n",
    "    def set_ms_run(self, peak_filenames: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Add input peak files to the mzTab metadata section.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        peak_filenames : List[str]\n",
    "            The input peak file name(s).\n",
    "        \"\"\"\n",
    "        for i, filename in enumerate(natsort.natsorted(peak_filenames), 1):\n",
    "            filename = os.path.abspath(filename)\n",
    "            self.metadata.append(\n",
    "                (f\"ms_run[{i}]-location\", Path(filename).as_uri()),\n",
    "            )\n",
    "            self._run_map[filename] = i\n",
    "\n",
    "    def save(self) -> None:\n",
    "        \"\"\"\n",
    "        Export the spectrum identifications to the mzTab file.\n",
    "        \"\"\"\n",
    "        with open(self.filename, \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f, delimiter=\"\\t\", lineterminator=os.linesep)\n",
    "            # Write metadata.\n",
    "            for row in self.metadata:\n",
    "                writer.writerow([\"MTD\", *row])\n",
    "            # Write PSMs.\n",
    "\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    \"PSH\",\n",
    "                    \"sequence\",\n",
    "                    \"PSM_ID\",\n",
    "                    \"accession\",\n",
    "                    \"unique\",\n",
    "                    \"database\",\n",
    "                    \"database_version\",\n",
    "                    \"search_engine\",\n",
    "                    \"search_engine_score[1]\",\n",
    "                    \"modifications\",\n",
    "                    \"retention_time\",\n",
    "                    \"charge\",\n",
    "                    \"exp_mass_to_charge\",\n",
    "                    \"calc_mass_to_charge\",\n",
    "                    \"spectra_ref\",\n",
    "                    \"pre\",\n",
    "                    \"post\",\n",
    "                    \"start\",\n",
    "                    \"end\",\n",
    "                    \"opt_ms_run[1]_aa_scores\",\n",
    "                ]\n",
    "            )\n",
    "            for i, psm in enumerate(\n",
    "                natsort.natsorted(self.psms, key=operator.itemgetter(1)), 1\n",
    "            ):\n",
    "                filename, idx = os.path.abspath(psm[1][0]), psm[1][1]\n",
    "                writer.writerow(\n",
    "                    [\n",
    "                        \"PSM\",\n",
    "                        psm[0],  # sequence\n",
    "                        i,  # PSM_ID\n",
    "                        \"null\",  # accession\n",
    "                        \"null\",  # unique\n",
    "                        \"null\",  # database\n",
    "                        \"null\",  # database_version\n",
    "                        f\"[MS, MS:1003281, Casanovo, notebook_version]\",  # search_engine\n",
    "                        psm[2],  # search_engine_score[1]\n",
    "                        # FIXME: Modifications should be specified as\n",
    "                        #  controlled vocabulary terms.\n",
    "                        \"null\",  # modifications\n",
    "                        # FIXME: Can we get the retention time from the data\n",
    "                        #  loader?\n",
    "                        \"null\",  # retention_time\n",
    "                        psm[3],  # charge\n",
    "                        psm[4],  # exp_mass_to_charge\n",
    "                        psm[5],  # calc_mass_to_charge\n",
    "                        f\"ms_run[{self._run_map[filename]}]:{idx}\",\n",
    "                        \"null\",  # pre\n",
    "                        \"null\",  # post\n",
    "                        \"null\",  # start\n",
    "                        \"null\",  # end\n",
    "                        psm[6],  # opt_ms_run[1]_aa_scores\n",
    "                    ]\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f4ba1a",
   "metadata": {},
   "source": [
    "**Raw/mgf or other data loading**\n",
    "\n",
    "casanovo默认调用depthcharge库读取raw/mgf，后期更换为alpharaw\n",
    "\n",
    "目前为数据格式为：depthcharge处理的AnnotatedSpectrumIndex文件，预处理好的谱图索引文件，如HDF5，用于数据加载\n",
    "AnnotatedSpectrumDataset:有标签数据；\n",
    "SpectrumDataset：无标签\n",
    "todo：暂时未真正加载raw，应该要与model_runner配合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41c323c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import depthcharge\n",
    "import numpy as np\n",
    "import spectrum_utils.spectrum as sus\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from depthcharge.data import AnnotatedSpectrumIndex\n",
    "import functools\n",
    "import os\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "logger = logging.getLogger(\"casanovo\")\n",
    "\n",
    "class SpectrumDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Parse and retrieve collections of MS/MS spectra.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spectrum_index : depthcharge.data.SpectrumIndex\n",
    "        The MS/MS spectra to use as a dataset.\n",
    "    n_peaks : Optional[int]\n",
    "        The number of top-n most intense peaks to keep in each spectrum. `None`\n",
    "        retains all peaks.\n",
    "    min_mz : float\n",
    "        The minimum m/z to include. The default is 140 m/z, in order to exclude\n",
    "        TMT and iTRAQ reporter ions.\n",
    "    max_mz : float\n",
    "        The maximum m/z to include.\n",
    "    min_intensity : float\n",
    "        Remove peaks whose intensity is below `min_intensity` percentage of the\n",
    "        base peak intensity.\n",
    "    remove_precursor_tol : float\n",
    "        Remove peaks within the given mass tolerance in Dalton around the\n",
    "        precursor mass.\n",
    "    random_state : Optional[int]\n",
    "        The NumPy random state. ``None`` leaves mass spectra in the order they\n",
    "        were parsed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spectrum_index: depthcharge.data.SpectrumIndex,\n",
    "        n_peaks: int = 150,\n",
    "        min_mz: float = 140.0,\n",
    "        max_mz: float = 2500.0,\n",
    "        min_intensity: float = 0.01,\n",
    "        remove_precursor_tol: float = 2.0,\n",
    "        random_state: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"Initialize a SpectrumDataset\"\"\"\n",
    "        super().__init__()\n",
    "        self.n_peaks = n_peaks\n",
    "        self.min_mz = min_mz\n",
    "        self.max_mz = max_mz\n",
    "        self.min_intensity = min_intensity\n",
    "        self.remove_precursor_tol = remove_precursor_tol\n",
    "        self.rng = np.random.default_rng(random_state)\n",
    "        self._index = spectrum_index #内部使用\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"The number of spectra.\"\"\"\n",
    "        return self.n_spectra\n",
    "\n",
    "    def __getitem__(\n",
    "        self, idx\n",
    "    ) -> Tuple[torch.Tensor, float, int, Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Return the MS/MS spectrum with the given index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            The index of the spectrum to return.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        spectrum : torch.Tensor of shape (n_peaks, 2)\n",
    "            A tensor of the spectrum with the m/z and intensity peak values.\n",
    "        precursor_mz : float\n",
    "            The precursor m/z.\n",
    "        precursor_charge : int\n",
    "            The precursor charge.\n",
    "        spectrum_id: Tuple[str, str]\n",
    "            The unique spectrum identifier, formed by its original peak file and\n",
    "            identifier (index or scan number) therein.\n",
    "        \"\"\"\n",
    "        mz_array, int_array, precursor_mz, precursor_charge = self.index[idx]\n",
    "        spectrum = self._process_peaks(\n",
    "            mz_array, int_array, precursor_mz, precursor_charge\n",
    "        )\n",
    "        return (\n",
    "            spectrum,\n",
    "            precursor_mz,\n",
    "            precursor_charge,\n",
    "            self.get_spectrum_id(idx),\n",
    "        )\n",
    "\n",
    "    def get_spectrum_id(self, idx: int) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Return the identifier of the MS/MS spectrum with the given index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            The index of the MS/MS spectrum within the SpectrumIndex.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ms_data_file : str\n",
    "            The peak file from which the MS/MS spectrum was originally parsed.\n",
    "        identifier : str\n",
    "            The MS/MS spectrum identifier, per PSI recommendations.\n",
    "        \"\"\"\n",
    "        with self.index:\n",
    "            return self.index.get_spectrum_id(idx) #调用self.index的get_spectrum_id\n",
    "\n",
    "    def _process_peaks(\n",
    "        self,\n",
    "        mz_array: np.ndarray,\n",
    "        int_array: np.ndarray,\n",
    "        precursor_mz: float,\n",
    "        precursor_charge: int,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Preprocess the spectrum by removing noise peaks and scaling the peak\n",
    "        intensities.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mz_array : numpy.ndarray of shape (n_peaks,)\n",
    "            The spectrum peak m/z values.\n",
    "        int_array : numpy.ndarray of shape (n_peaks,)\n",
    "            The spectrum peak intensity values.\n",
    "        precursor_mz : float\n",
    "            The precursor m/z.\n",
    "        precursor_charge : int\n",
    "            The precursor charge.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor of shape (n_peaks, 2)\n",
    "            A tensor of the spectrum with the m/z and intensity peak values.\n",
    "        \"\"\"\n",
    "        spectrum = sus.MsmsSpectrum(\n",
    "            \"\",\n",
    "            precursor_mz,\n",
    "            precursor_charge,\n",
    "            mz_array.astype(np.float64),\n",
    "            int_array.astype(np.float32),\n",
    "        )\n",
    "        try:\n",
    "            spectrum.set_mz_range(self.min_mz, self.max_mz)\n",
    "            if len(spectrum.mz) == 0:\n",
    "                raise ValueError\n",
    "            spectrum.remove_precursor_peak(self.remove_precursor_tol, \"Da\")\n",
    "            if len(spectrum.mz) == 0:\n",
    "                raise ValueError\n",
    "            spectrum.filter_intensity(self.min_intensity, self.n_peaks)\n",
    "            if len(spectrum.mz) == 0:\n",
    "                raise ValueError\n",
    "            spectrum.scale_intensity(\"root\", 1)\n",
    "            intensities = spectrum.intensity / np.linalg.norm(\n",
    "                spectrum.intensity\n",
    "            )\n",
    "            return torch.tensor(np.array([spectrum.mz, intensities])).T.float()\n",
    "        except ValueError:\n",
    "            # Replace invalid spectra by a dummy spectrum.\n",
    "            return torch.tensor([[0, 1]]).float()\n",
    "\n",
    "    @property #当作属性调用：dataset.n_spectra;简洁、让外部不知道你做的是函数操作\n",
    "    def n_spectra(self) -> int:\n",
    "        \"\"\"The total number of spectra.\"\"\"\n",
    "        return self.index.n_spectra\n",
    "\n",
    "    @property\n",
    "    def index(self) -> depthcharge.data.SpectrumIndex:\n",
    "        \"\"\"The underlying SpectrumIndex.\"\"\"\n",
    "        return self._index\n",
    "\n",
    "    @property\n",
    "    def rng(self):\n",
    "        \"\"\"The NumPy random number generator.\"\"\"\n",
    "        return self._rng\n",
    "\n",
    "    @rng.setter\n",
    "    def rng(self, seed):\n",
    "        \"\"\"Set the NumPy random number generator.\"\"\"\n",
    "        self._rng = np.random.default_rng(seed)\n",
    "\n",
    "class AnnotatedSpectrumDataset(SpectrumDataset):\n",
    "    \"\"\"\n",
    "    Parse and retrieve collections of annotated MS/MS spectra.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    annotated_spectrum_index : depthcharge.data.SpectrumIndex\n",
    "        The MS/MS spectra to use as a dataset.\n",
    "    n_peaks : Optional[int]\n",
    "        The number of top-n most intense peaks to keep in each spectrum. `None`\n",
    "        retains all peaks.\n",
    "    min_mz : float\n",
    "        The minimum m/z to include. The default is 140 m/z, in order to exclude\n",
    "        TMT and iTRAQ reporter ions.\n",
    "    max_mz : float\n",
    "        The maximum m/z to include.\n",
    "    min_intensity : float\n",
    "        Remove peaks whose intensity is below `min_intensity` percentage of the\n",
    "        base peak intensity.\n",
    "    remove_precursor_tol : float\n",
    "        Remove peaks within the given mass tolerance in Dalton around the\n",
    "        precursor mass.\n",
    "    random_state : Optional[int]\n",
    "        The NumPy random state. ``None`` leaves mass spectra in the order they\n",
    "        were parsed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        annotated_spectrum_index: depthcharge.data.SpectrumIndex,\n",
    "        n_peaks: int = 150,\n",
    "        min_mz: float = 140.0,\n",
    "        max_mz: float = 2500.0,\n",
    "        min_intensity: float = 0.01,\n",
    "        remove_precursor_tol: float = 2.0,\n",
    "        random_state: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            annotated_spectrum_index,\n",
    "            n_peaks=n_peaks,\n",
    "            min_mz=min_mz,\n",
    "            max_mz=max_mz,\n",
    "            min_intensity=min_intensity,\n",
    "            remove_precursor_tol=remove_precursor_tol,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, float, int, str]:\n",
    "        \"\"\"\n",
    "        Return the annotated MS/MS spectrum with the given index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            The index of the spectrum to return.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        spectrum : torch.Tensor of shape (n_peaks, 2)\n",
    "            A tensor of the spectrum with the m/z and intensity peak values.\n",
    "        precursor_mz : float\n",
    "            The precursor m/z.\n",
    "        precursor_charge : int\n",
    "            The precursor charge.\n",
    "        annotation : str\n",
    "            The peptide annotation of the spectrum.\n",
    "        \"\"\"\n",
    "        (\n",
    "            mz_array,\n",
    "            int_array,\n",
    "            precursor_mz,\n",
    "            precursor_charge,\n",
    "            peptide,\n",
    "        ) = self.index[idx]\n",
    "        spectrum = self._process_peaks(\n",
    "            mz_array, int_array, precursor_mz, precursor_charge\n",
    "        )\n",
    "        return spectrum, precursor_mz, precursor_charge, peptide\n",
    "\n",
    "class DeNovoDataModule(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    Data loader to prepare MS/MS spectra for a Spec2Pep predictor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_index : Optional[AnnotatedSpectrumIndex]\n",
    "        The spectrum index file corresponding to the training data.\n",
    "    valid_index : Optional[AnnotatedSpectrumIndex]\n",
    "        The spectrum index file corresponding to the validation data.\n",
    "    test_index : Optional[AnnotatedSpectrumIndex]\n",
    "        The spectrum index file corresponding to the testing data.\n",
    "    train_batch_size : int\n",
    "        The batch size to use for training.\n",
    "    eval_batch_size : int\n",
    "        The batch size to use for inference.\n",
    "    n_peaks : Optional[int]\n",
    "        The number of top-n most intense peaks to keep in each spectrum. `None`\n",
    "        retains all peaks.\n",
    "    min_mz : float\n",
    "        The minimum m/z to include. The default is 140 m/z, in order to exclude\n",
    "        TMT and iTRAQ reporter ions.\n",
    "    max_mz : float\n",
    "        The maximum m/z to include.\n",
    "    min_intensity : float\n",
    "        Remove peaks whose intensity is below `min_intensity` percentage of the\n",
    "        base peak intensity.\n",
    "    remove_precursor_tol : float\n",
    "        Remove peaks within the given mass tolerance in Dalton around the\n",
    "        precursor mass.\n",
    "    n_workers : int, optional\n",
    "        The number of workers to use for data loading. By default, the number of\n",
    "        available CPU cores on the current machine is used.\n",
    "    random_state : Optional[int]\n",
    "        The NumPy random state. ``None`` leaves mass spectra in the order they\n",
    "        were parsed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_index: Optional[AnnotatedSpectrumIndex] = None,\n",
    "        valid_index: Optional[AnnotatedSpectrumIndex] = None,\n",
    "        test_index: Optional[AnnotatedSpectrumIndex] = None,\n",
    "        train_batch_size: int = 128,\n",
    "        eval_batch_size: int = 1028,\n",
    "        n_peaks: Optional[int] = 150,\n",
    "        min_mz: float = 50.0,\n",
    "        max_mz: float = 2500.0,\n",
    "        min_intensity: float = 0.01,\n",
    "        remove_precursor_tol: float = 2.0,\n",
    "        n_workers: Optional[int] = None,\n",
    "        random_state: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_index = train_index\n",
    "        self.valid_index = valid_index\n",
    "        self.test_index = test_index\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "        self.n_peaks = n_peaks\n",
    "        self.min_mz = min_mz\n",
    "        self.max_mz = max_mz\n",
    "        self.min_intensity = min_intensity\n",
    "        self.remove_precursor_tol = remove_precursor_tol\n",
    "        self.n_workers = n_workers if n_workers is not None else os.cpu_count()\n",
    "        self.rng = np.random.default_rng(random_state)\n",
    "        self.train_dataset = None\n",
    "        self.valid_dataset = None\n",
    "        self.test_dataset = None\n",
    "\n",
    "    def setup(self, stage: str = None, annotated: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Set up the PyTorch Datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        stage : str {\"fit\", \"validate\", \"test\"}\n",
    "            The stage indicating which Datasets to prepare. All are prepared by\n",
    "            default.\n",
    "        annotated: bool\n",
    "            True if peptide sequence annotations are available for the test\n",
    "            data.\n",
    "        \"\"\"\n",
    "        if stage in (None, \"fit\", \"validate\"): #使用partial，固定部分不变参数\n",
    "            make_dataset = functools.partial(\n",
    "                AnnotatedSpectrumDataset,\n",
    "                n_peaks=self.n_peaks,\n",
    "                min_mz=self.min_mz,\n",
    "                max_mz=self.max_mz,\n",
    "                min_intensity=self.min_intensity,\n",
    "                remove_precursor_tol=self.remove_precursor_tol,\n",
    "            )\n",
    "            if self.train_index is not None:\n",
    "                self.train_dataset = make_dataset(\n",
    "                    self.train_index,\n",
    "                    random_state=self.rng,\n",
    "                )\n",
    "            if self.valid_index is not None:\n",
    "                self.valid_dataset = make_dataset(self.valid_index)\n",
    "        if stage in (None, \"test\"):\n",
    "            make_dataset = functools.partial(\n",
    "                AnnotatedSpectrumDataset if annotated else SpectrumDataset,\n",
    "                n_peaks=self.n_peaks,\n",
    "                min_mz=self.min_mz,\n",
    "                max_mz=self.max_mz,\n",
    "                min_intensity=self.min_intensity,\n",
    "                remove_precursor_tol=self.remove_precursor_tol,\n",
    "            )\n",
    "            if self.test_index is not None:\n",
    "                self.test_dataset = make_dataset(self.test_index)\n",
    "\n",
    "    def _make_loader(\n",
    "        self,\n",
    "        dataset: torch.utils.data.Dataset,\n",
    "        batch_size: int,\n",
    "        shuffle: bool = False,\n",
    "    ) -> torch.utils.data.DataLoader:\n",
    "        \"\"\"\n",
    "        Create a PyTorch DataLoader.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : torch.utils.data.Dataset\n",
    "            A PyTorch Dataset.\n",
    "        batch_size : int\n",
    "            The batch size to use.\n",
    "        shuffle : bool\n",
    "            Option to shuffle the batches.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.utils.data.DataLoader\n",
    "            A PyTorch DataLoader.\n",
    "        \"\"\"\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=prepare_batch,\n",
    "            pin_memory=True,\n",
    "            num_workers=self.n_workers,\n",
    "            shuffle=shuffle,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        \"\"\"Get the training DataLoader.\"\"\"\n",
    "        return self._make_loader(\n",
    "            self.train_dataset, self.train_batch_size, shuffle=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        \"\"\"Get the validation DataLoader.\"\"\"\n",
    "        return self._make_loader(self.valid_dataset, self.eval_batch_size)\n",
    "\n",
    "    def test_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        \"\"\"Get the test DataLoader.\"\"\"\n",
    "        return self._make_loader(self.test_dataset, self.eval_batch_size)\n",
    "\n",
    "    def predict_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        \"\"\"Get the predict DataLoader.\"\"\"\n",
    "        return self._make_loader(self.test_dataset, self.eval_batch_size)\n",
    "\n",
    "def prepare_batch(\n",
    "    batch: List[Tuple[torch.Tensor, float, int, str]]\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Collate MS/MS spectra into a batch.\n",
    "\n",
    "    The MS/MS spectra will be padded so that they fit nicely as a tensor.\n",
    "    However, the padded elements are ignored during the subsequent steps.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch : List[Tuple[torch.Tensor, float, int, str]]\n",
    "        A batch of data from an AnnotatedSpectrumDataset, consisting of for each\n",
    "        spectrum (i) a tensor with the m/z and intensity peak values, (ii), the\n",
    "        precursor m/z, (iii) the precursor charge, (iv) the spectrum identifier.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    spectra : torch.Tensor of shape (batch_size, n_peaks, 2)\n",
    "        The padded mass spectra tensor with the m/z and intensity peak values\n",
    "        for each spectrum.\n",
    "    precursors : torch.Tensor of shape (batch_size, 3)\n",
    "        A tensor with the precursor neutral mass, precursor charge, and\n",
    "        precursor m/z.\n",
    "    spectrum_ids : np.ndarray\n",
    "        The spectrum identifiers (during de novo sequencing) or peptide\n",
    "        sequences (during training).\n",
    "    \"\"\"\n",
    "    spectra, precursor_mzs, precursor_charges, spectrum_ids = list(zip(*batch))\n",
    "    spectra = torch.nn.utils.rnn.pad_sequence(spectra, batch_first=True)\n",
    "    precursor_mzs = torch.tensor(precursor_mzs)\n",
    "    precursor_charges = torch.tensor(precursor_charges)\n",
    "    precursor_masses = (precursor_mzs - 1.007276) * precursor_charges\n",
    "    precursors = torch.vstack(\n",
    "        [precursor_masses, precursor_charges, precursor_mzs]\n",
    "    ).T.float()\n",
    "    return spectra, precursors, np.asarray(spectrum_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd7a1b2",
   "metadata": {},
   "source": [
    "**hdf data loader:修改自DeNovoDataModule**\n",
    "\n",
    "适配alphabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3cff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphabase.io.hdf import HDF_File\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HDFSpectrumDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hdf5_path: str,\n",
    "        n_peaks: int = 150,\n",
    "        min_mz: float = 140.0,\n",
    "        max_mz: float = 2500.0,\n",
    "        min_intensity: float = 0.01,\n",
    "        remove_precursor_tol: float = 2.0,\n",
    "        random_state: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"Initialize HDF5 dataset\"\"\"\n",
    "        super().__init__()\n",
    "        self.n_peaks = n_peaks\n",
    "        self.min_mz = min_mz\n",
    "        self.max_mz = max_mz\n",
    "        self.min_intensity = min_intensity\n",
    "        self.remove_precursor_tol = remove_precursor_tol\n",
    "        self.rng = np.random.default_rng(random_state)\n",
    "\n",
    "        #Load HDF5 file\n",
    "        self.hdf5_file = HDF_File(hdf5_path, read_only=True)\n",
    "        self.psm_df = self.hdf5_file.get_dataframe(\"psm_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e9d9c1",
   "metadata": {},
   "source": [
    "**hdf data loader**\n",
    "目前：训练读取带标签hdf文件，可分批次处理大量训练数据（搁置）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bacb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import h5py\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "from alphabase.io.hdf import HDF_File\n",
    "\n",
    "# =============================================================================\n",
    "# 1. 自定义 Dataset：从多个 HDF5 文件中按需分批加载\n",
    "# =============================================================================\n",
    "class HDF5Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    一个能处理多文件、大规模 HDF5 数据集的 PyTorch Dataset。\n",
    "    数据假设在每个 HDF5 文件里按照相同 schema 存储：\n",
    "      - 'sequences'  : shape = (N, ...)，存放每条样本的 seq （可变长度）\n",
    "      - 'coords'     : shape = (N, ...) ，存放每条样本的 coords（固定形状）\n",
    "      - 'pdb_ids'    : shape = (N,) 或 (N, some_len) ，存放每条样本的 pdb_id（字符串或编码）\n",
    "    具体字段名可根据实际情况修改。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_paths, cache_size=4, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          file_paths (List[str]): 所有待加载的 HDF5 文件路径列表。\n",
    "          cache_size (int): 同时保持在内存的 HDF5 打开文件句柄数。超过这个数时会按照 LRU 关闭最久未访问的文件。\n",
    "          transform (callable, optional): 对 (spec_seq, seq_mask) 进行可选的数据增强/预处理。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.file_paths = file_paths\n",
    "        self.cache_size = cache_size\n",
    "        self.transform = transform\n",
    "\n",
    "        # 1) 统计每个文件内的样本数，并累计总样本数\n",
    "        self.file_sizes = []\n",
    "        total = 0\n",
    "        for path in self.file_paths:\n",
    "            raw_hdf_file = HDF_File(file_name = path, read_only = True)    \n",
    "            num = len(raw_hdf_file.psm.psm_df)\n",
    "            self.file_sizes.append(num)\n",
    "            total += num\n",
    "        self.size = total  # 整个数据集的总样本数\n",
    "\n",
    "        # 2) 构建 “全局 idx -> (file_idx, local_idx)” 的映射表\n",
    "        #    例如：若第 0 个文件有 100 条，第 1 个文件有 150 条，\n",
    "        #    那么 idx 0..99 映射到 (0, 0..99)、idx 100..249 映射到 (1, 0..149)，以此类推。\n",
    "        self.index_map = []\n",
    "        for file_idx, num in enumerate(self.file_sizes):\n",
    "            for local_idx in range(num):\n",
    "                self.index_map.append((file_idx, local_idx))\n",
    "        assert len(self.index_map) == self.size\n",
    "\n",
    "        # 3) LRU Cache：维护当前打开的 h5py.File 对象\n",
    "        #    OrderedDict 能保证“最近访问的 key 被移到末尾”，我们 popitem(last=False) 来删除最旧的。\n",
    "        self._cache = OrderedDict()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def _open_file(self, file_path):\n",
    "        \"\"\"\n",
    "        在内部使用 LRU 逻辑打开或获取一个 HDF5 文件。\n",
    "        如果 cache 达到上限，会先关闭并移除最久未使用的文件。\n",
    "        返回：已经打开的 h5py.File 对象\n",
    "        \"\"\"\n",
    "        # 如果已经在缓存里，先把它移动到末尾（表示“刚使用过”）\n",
    "        if file_path in self._cache:\n",
    "            self._cache.move_to_end(file_path)\n",
    "            return self._cache[file_path]\n",
    "\n",
    "        # 不在缓存里，则要打开新文件\n",
    "        if len(self._cache) >= self.cache_size:\n",
    "            # popitem(last=False) 会删除 OrderedDict 最前面的 key（最旧访问的）\n",
    "            oldest_path, oldest_file = self._cache.popitem(last=False)\n",
    "            try:\n",
    "                oldest_file.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # 打开新文件并放到缓存末尾\n",
    "        f = HDF_File(file_name=file_path, read_only=True)\n",
    "        self._cache[file_path] = f  \n",
    "        return f\n",
    "\n",
    "    def _extract_psm(self,f):\n",
    "        \"\"\"从hdf5文件中提取psm数据:\n",
    "        \"\"\"\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        根据全局 idx 返回单条样本的数据字典：\n",
    "          {\n",
    "            'seq':     Tensor or numpy array, 可能是一维、可变长度\n",
    "            'coords':  Tensor or numpy array, 固定 shape\n",
    "            'pdb_id':  str 或者独热向量\n",
    "          }\n",
    "        \"\"\"\n",
    "        if idx < 0 or idx >= self.size:\n",
    "            raise IndexError(f\"Index {idx} 越界，允许范围 [0, {self.size-1}]\")\n",
    "\n",
    "        file_idx, local_idx = self.index_map[idx]\n",
    "        file_path = self.file_paths[file_idx]\n",
    "\n",
    "        # 从 LRU 缓存或新打开\n",
    "        f = self._open_file(file_path)\n",
    "\n",
    "        # 读取数据 —— 根据实际 HDF5 schema 修改 key 名称\n",
    "        seq_np    = f['sequences'][local_idx]  # numpy array 或 bytes/string\n",
    "        coords_np = f['coords'][local_idx]\n",
    "        pdb_id_np = f['pdb_ids'][local_idx]    # 假设是 bytes 或者字符串编码\n",
    "\n",
    "        # 如果需要对 seq 做数据增强，就转换为 torch.Tensor，然后调用 transform\n",
    "        seq = torch.tensor(seq_np, dtype=torch.float)      # [T], T 可变\n",
    "        coords = torch.tensor(coords_np, dtype=torch.float)  # e.g. [T, F]\n",
    "        # pdb_id 一般先保持原样，后续如果需要可以转换为编码或直接打印\n",
    "        pdb_id = pdb_id_np if isinstance(pdb_id_np, str) else pdb_id_np.decode('utf-8')\n",
    "\n",
    "        # 可选的数据预处理 / 增强\n",
    "        if self.transform is not None:\n",
    "            seq, coords = self.transform(seq, coords)\n",
    "\n",
    "        return {\n",
    "            'seq': seq,\n",
    "            'coords': coords,\n",
    "            'pdb_id': pdb_id\n",
    "        }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. 自定义 collate_fn：将多个“一条样本” 组合成一个 batch\n",
    "# =============================================================================\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: 是一个 list，大约长度 = batch_size，\n",
    "           每个元素是 HDF5Dataset.__getitem__ 返回的 dict:\n",
    "             {\n",
    "                'seq': Tensor [T_i],\n",
    "                'coords': Tensor [T_i, F],\n",
    "                'pdb_id': str\n",
    "             }\n",
    "    这里要做的：\n",
    "      1. 对所有 seq 做 padding → 得到 [B, T_max]\n",
    "      2. 对所有 coords 做 stack → 得到 [B, T_max, F]，对 padding 部分可以填 0\n",
    "      3. 记录原始长度 list_of_lengths = [T_1, T_2, ..., T_B]\n",
    "      4. 把 pdb_ids 收集成一个 Python list\n",
    "    返回一个 dict:\n",
    "      {\n",
    "         'sequences': padded_seq_tensor,  # [B, T_max]\n",
    "         'coords':      padded_coords,    # [B, T_max, F]\n",
    "         'lengths':     torch.LongTensor([T_1, ..., T_B]),  # [B]\n",
    "         'pdb_ids':     [str1, str2, ..., strB]\n",
    "      }\n",
    "    \"\"\"\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    # 1) 先分别取出 seq、coords、pdb_id\n",
    "    seqs    = [item['seq'] for item in batch]\n",
    "    coords  = [item['coords'] for item in batch]\n",
    "    pdb_ids = [item['pdb_id'] for item in batch]\n",
    "\n",
    "    # 2) 计算每个样本的时序长度\n",
    "    lengths = [s.size(0) for s in seqs]       # [T_1, T_2, ..., T_B]\n",
    "    T_max = max(lengths)\n",
    "\n",
    "    # 3) 在时序维度上对 seqs 做 padding\n",
    "    #    最终 shape: [B, T_max]\n",
    "    padded_seqs = torch.zeros((batch_size, T_max), dtype=torch.float)\n",
    "    for i, s in enumerate(seqs):\n",
    "        L = s.size(0)\n",
    "        padded_seqs[i, :L] = s\n",
    "\n",
    "    # 4) 在时序维度和 feature 维度上对 coords 做 padding\n",
    "    #    假设 coords[i] 的 shape = [T_i, F]\n",
    "    F_dim = coords[0].size(1)\n",
    "    padded_coords = torch.zeros((batch_size, T_max, F_dim), dtype=torch.float)\n",
    "    for i, c in enumerate(coords):\n",
    "        L = c.size(0)\n",
    "        padded_coords[i, :L, :] = c\n",
    "\n",
    "    # 5) 把 lengths 转成 LongTensor\n",
    "    lengths_tensor = torch.LongTensor(lengths)  # [B]\n",
    "\n",
    "    return {\n",
    "        'sequences': padded_seqs,   # [B, T_max]\n",
    "        'coords': padded_coords,    # [B, T_max, F_dim]\n",
    "        'lengths': lengths_tensor,  # [B]\n",
    "        'pdb_ids': pdb_ids         # List[str]\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. 将样本文件列表划分成 train / val\n",
    "# =============================================================================\n",
    "def create_data_splits(hdf5_files, val_ratio=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    随机打乱后按比例划分文件列表：\n",
    "      - hdf5_files: List[str]，全量文件路径\n",
    "      - val_ratio: 验证集占比，例如 0.2 → 20%\n",
    "      - seed: 随机种子，保证可复现\n",
    "\n",
    "    返回:\n",
    "      train_files, val_files: 两个列表\n",
    "    \"\"\"\n",
    "    files = hdf5_files.copy()\n",
    "    random.seed(seed)\n",
    "    random.shuffle(files)\n",
    "\n",
    "    split_idx = int(len(files) * (1 - val_ratio))\n",
    "    train_files = files[:split_idx]\n",
    "    val_files   = files[split_idx:]\n",
    "\n",
    "    return train_files, val_files\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. PyTorch Lightning DataModule：负责数据准备/加载\n",
    "# =============================================================================\n",
    "class HDF5DataModule(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    LightningDataModule 将覆盖以下方法：\n",
    "      - setup(stage): 负责“划分数据集并创建 Dataset” 的逻辑\n",
    "      - train_dataloader(): 返回训练集的 DataLoader\n",
    "      - val_dataloader(): 返回验证集的 DataLoader\n",
    "      - (可选) test_dataloader(): 返回测试集的 DataLoader\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, batch_size=32, val_ratio=0.2, num_workers=4, cache_size=4, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          data_dir (str): 存放所有 .hdf5 文件的目录路径\n",
    "          batch_size (int): 每个 batch 的样本数\n",
    "          val_ratio (float): 验证集占比，例如 0.2\n",
    "          num_workers (int): DataLoader 并行加载时的子进程数\n",
    "          cache_size (int): LRU 缓存最大打开文件数\n",
    "          transform (callable, optional): 传给 Dataset 的数据预处理/增强函数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_dir    = data_dir\n",
    "        self.batch_size  = batch_size\n",
    "        self.val_ratio   = val_ratio\n",
    "        self.num_workers = num_workers\n",
    "        self.cache_size  = cache_size\n",
    "        self.transform   = transform\n",
    "\n",
    "        # 在 setup() 中会被赋值：\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset   = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"\n",
    "        Lightning 在 fit() 之前会调用一次 setup(\"fit\")，可以在这里准备 train/val 的 Dataset。\n",
    "        stage 参数可为 \"fit\", \"validate\", \"test\", \"predict\" 等，但这里只需要在 \"fit\" 时构造训练/验证集。\n",
    "        \"\"\"\n",
    "        # 1) 找到所有 .hdf5 文件\n",
    "        pattern = os.path.join(self.data_dir, \"*.hdf5\")\n",
    "        all_files = glob.glob(pattern)\n",
    "        if len(all_files) == 0:\n",
    "            raise FileNotFoundError(f\"在目录 {self.data_dir} 中未找到任何 .hdf5 文件\")\n",
    "\n",
    "        # 2) 划分 train / val 文件列表\n",
    "        train_files, val_files = create_data_splits(all_files, self.val_ratio)\n",
    "\n",
    "        # 3) 分别创建 Dataset\n",
    "        self.train_dataset = HDF5Dataset(\n",
    "            file_paths=train_files,\n",
    "            cache_size=self.cache_size,\n",
    "            transform=self.transform\n",
    "        )\n",
    "        self.val_dataset = HDF5Dataset(\n",
    "            file_paths=val_files,\n",
    "            cache_size=self.cache_size,\n",
    "            transform=None  # 验证/测试阶段一般不做随机增强\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        训练时使用的 DataLoader：会自动从 self.train_dataset 中分批读取，\n",
    "        使用我们定义的 collate_fn 做 batch 拼装，使用多个 num_workers 预取。\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,               # 训练时打乱\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"\n",
    "        验证时使用的 DataLoader：不打乱，batch_size 与训练一致\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "    # 如果需要测试集，也可以定义：\n",
    "    # def test_dataloader(self):\n",
    "    #     return DataLoader(...)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. 示例：在 Lightning Trainer 中如何使用\n",
    "# =============================================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 1) 配置数据目录和参数\n",
    "#     data_dir = \"/path/to/hdf5/files\"   # TODO: 修改为你的数据目录\n",
    "#     batch_size = 32\n",
    "#     val_ratio = 0.2\n",
    "#     num_workers = 4\n",
    "#     cache_size = 4\n",
    "\n",
    "\n",
    "\n",
    "#     # 2) 创建 DataModule\n",
    "#     data_module = HDF5DataModule(\n",
    "#         data_dir=data_dir,\n",
    "#         batch_size=batch_size,\n",
    "#         val_ratio=val_ratio,\n",
    "#         num_workers=num_workers,\n",
    "#         cache_size=cache_size,\n",
    "#         transform=example_transform\n",
    "#     )\n",
    "\n",
    "#     # 3) 创建你的 LightningModule（这里示例一个空壳）\n",
    "\n",
    "#     model = YourModel()\n",
    "\n",
    "#     # 4) 启动训练\n",
    "#     trainer = pl.Trainer(\n",
    "#         max_epochs=10,\n",
    "#         gpus=1 if torch.cuda.is_available() else 0,\n",
    "#         progress_bar_refresh_rate=20\n",
    "#     )\n",
    "#     trainer.fit(model, datamodule=data_module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a233e1a9",
   "metadata": {},
   "source": [
    "**Evaluate**\n",
    "\n",
    "评估肽段-谱图匹配函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3a987f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Methods to evaluate peptide-spectrum predictions.\"\"\"\n",
    "\n",
    "import re\n",
    "from typing import Dict, Iterable, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from spectrum_utils.utils import mass_diff\n",
    "\n",
    "logger = logging.getLogger(\"casanovo\")\n",
    "#前缀匹配\n",
    "def aa_match_prefix(\n",
    "    peptide1: List[str],\n",
    "    peptide2: List[str],\n",
    "    aa_dict: Dict[str, float],\n",
    "    cum_mass_threshold: float = 0.5,\n",
    "    ind_mass_threshold: float = 0.1,\n",
    ") -> Tuple[np.ndarray, bool]:\n",
    "    \"\"\"\n",
    "    Find the matching prefix amino acids between two peptide sequences.\n",
    "\n",
    "    This is a similar evaluation criterion as used by DeepNovo.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    peptide1 : List[str]\n",
    "        The first tokenized peptide sequence to be compared.\n",
    "    peptide2 : List[str]\n",
    "        The second tokenized peptide sequence to be compared.\n",
    "    aa_dict : Dict[str, float]\n",
    "        Mapping of amino acid tokens to their mass values.\n",
    "    cum_mass_threshold : float\n",
    "        Mass threshold in Dalton to accept cumulative mass-matching amino acid\n",
    "        sequences.\n",
    "    ind_mass_threshold : float\n",
    "        Mass threshold in Dalton to accept individual mass-matching amino acids.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    aa_matches : np.ndarray of length max(len(peptide1), len(peptide2))\n",
    "        Boolean flag indicating whether each paired-up amino acid matches across\n",
    "        both peptide sequences.\n",
    "    pep_match : bool\n",
    "        Boolean flag to indicate whether the two peptide sequences fully match.\n",
    "    \"\"\"\n",
    "    aa_matches = np.zeros(max(len(peptide1), len(peptide2)), np.bool_)\n",
    "    # Find longest mass-matching prefix.\n",
    "    i1, i2, cum_mass1, cum_mass2 = 0, 0, 0.0, 0.0\n",
    "    while i1 < len(peptide1) and i2 < len(peptide2):\n",
    "        aa_mass1 = aa_dict.get(peptide1[i1], 0)\n",
    "        aa_mass2 = aa_dict.get(peptide2[i2], 0)\n",
    "        if (\n",
    "            abs(mass_diff(cum_mass1 + aa_mass1, cum_mass2 + aa_mass2, True))\n",
    "            < cum_mass_threshold\n",
    "        ):\n",
    "            aa_matches[max(i1, i2)] = (\n",
    "                abs(mass_diff(aa_mass1, aa_mass2, True)) < ind_mass_threshold\n",
    "            )\n",
    "            i1, i2 = i1 + 1, i2 + 1\n",
    "            cum_mass1, cum_mass2 = cum_mass1 + aa_mass1, cum_mass2 + aa_mass2\n",
    "        elif cum_mass2 + aa_mass2 > cum_mass1 + aa_mass1:\n",
    "            i1, cum_mass1 = i1 + 1, cum_mass1 + aa_mass1\n",
    "        else:\n",
    "            i2, cum_mass2 = i2 + 1, cum_mass2 + aa_mass2\n",
    "    return aa_matches, aa_matches.all()\n",
    "\n",
    "#前后缀匹配\n",
    "def aa_match_prefix_suffix(\n",
    "    peptide1: List[str],\n",
    "    peptide2: List[str],\n",
    "    aa_dict: Dict[str, float],\n",
    "    cum_mass_threshold: float = 0.5,\n",
    "    ind_mass_threshold: float = 0.1,\n",
    ") -> Tuple[np.ndarray, bool]:\n",
    "    \"\"\"\n",
    "    Find the matching prefix and suffix amino acids between two peptide\n",
    "    sequences.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    peptide1 : List[str]\n",
    "        The first tokenized peptide sequence to be compared.\n",
    "    peptide2 : List[str]\n",
    "        The second tokenized peptide sequence to be compared.\n",
    "    aa_dict : Dict[str, float]\n",
    "        Mapping of amino acid tokens to their mass values.\n",
    "    cum_mass_threshold : float\n",
    "        Mass threshold in Dalton to accept cumulative mass-matching amino acid\n",
    "        sequences.\n",
    "    ind_mass_threshold : float\n",
    "        Mass threshold in Dalton to accept individual mass-matching amino acids.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    aa_matches : np.ndarray of length max(len(peptide1), len(peptide2))\n",
    "        Boolean flag indicating whether each paired-up amino acid matches across\n",
    "        both peptide sequences.\n",
    "    pep_match : bool\n",
    "        Boolean flag to indicate whether the two peptide sequences fully match.\n",
    "    \"\"\"\n",
    "    # Find longest mass-matching prefix.\n",
    "    aa_matches, pep_match = aa_match_prefix(\n",
    "        peptide1, peptide2, aa_dict, cum_mass_threshold, ind_mass_threshold\n",
    "    )\n",
    "    # No need to evaluate the suffixes if the sequences already fully match.\n",
    "    if pep_match:\n",
    "        return aa_matches, pep_match\n",
    "    # Find longest mass-matching suffix.\n",
    "    i1, i2 = len(peptide1) - 1, len(peptide2) - 1\n",
    "    i_stop = np.argwhere(~aa_matches)[0]\n",
    "    cum_mass1, cum_mass2 = 0.0, 0.0\n",
    "    while i1 >= i_stop and i2 >= i_stop:\n",
    "        aa_mass1 = aa_dict.get(peptide1[i1], 0)\n",
    "        aa_mass2 = aa_dict.get(peptide2[i2], 0)\n",
    "        if (\n",
    "            abs(mass_diff(cum_mass1 + aa_mass1, cum_mass2 + aa_mass2, True))\n",
    "            < cum_mass_threshold\n",
    "        ):\n",
    "            aa_matches[max(i1, i2)] = (\n",
    "                abs(mass_diff(aa_mass1, aa_mass2, True)) < ind_mass_threshold\n",
    "            )\n",
    "            i1, i2 = i1 - 1, i2 - 1\n",
    "            cum_mass1, cum_mass2 = cum_mass1 + aa_mass1, cum_mass2 + aa_mass2\n",
    "        elif cum_mass2 + aa_mass2 > cum_mass1 + aa_mass1:\n",
    "            i1, cum_mass1 = i1 - 1, cum_mass1 + aa_mass1\n",
    "        else:\n",
    "            i2, cum_mass2 = i2 - 1, cum_mass2 + aa_mass2\n",
    "    return aa_matches, aa_matches.all()\n",
    "\n",
    "#多模式匹配\n",
    "def aa_match(\n",
    "    peptide1: List[str],\n",
    "    peptide2: List[str],\n",
    "    aa_dict: Dict[str, float],\n",
    "    cum_mass_threshold: float = 0.5,\n",
    "    ind_mass_threshold: float = 0.1,\n",
    "    mode: str = \"best\",\n",
    ") -> Tuple[np.ndarray, bool]:\n",
    "    \"\"\"\n",
    "    Find the matching amino acids between two peptide sequences.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    peptide1 : List[str]\n",
    "        The first tokenized peptide sequence to be compared.\n",
    "    peptide2 : List[str]\n",
    "        The second tokenized peptide sequence to be compared.\n",
    "    aa_dict : Dict[str, float]\n",
    "        Mapping of amino acid tokens to their mass values.\n",
    "    cum_mass_threshold : float\n",
    "        Mass threshold in Dalton to accept cumulative mass-matching amino acid\n",
    "        sequences.\n",
    "    ind_mass_threshold : float\n",
    "        Mass threshold in Dalton to accept individual mass-matching amino acids.\n",
    "    mode : {\"best\", \"forward\", \"backward\"}\n",
    "        The direction in which to find matching amino acids.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    aa_matches : np.ndarray of length max(len(peptide1), len(peptide2))\n",
    "        Boolean flag indicating whether each paired-up amino acid matches across\n",
    "        both peptide sequences.\n",
    "    pep_match : bool\n",
    "        Boolean flag to indicate whether the two peptide sequences fully match.\n",
    "    \"\"\"\n",
    "    if mode == \"best\":\n",
    "        return aa_match_prefix_suffix(\n",
    "            peptide1, peptide2, aa_dict, cum_mass_threshold, ind_mass_threshold\n",
    "        )\n",
    "    elif mode == \"forward\":\n",
    "        return aa_match_prefix(\n",
    "            peptide1, peptide2, aa_dict, cum_mass_threshold, ind_mass_threshold\n",
    "        )\n",
    "    elif mode == \"backward\":\n",
    "        aa_matches, pep_match = aa_match_prefix(\n",
    "            list(reversed(peptide1)),\n",
    "            list(reversed(peptide2)),\n",
    "            aa_dict,\n",
    "            cum_mass_threshold,\n",
    "            ind_mass_threshold,\n",
    "        )\n",
    "        return aa_matches[::-1], pep_match\n",
    "    else:\n",
    "        raise ValueError(\"Unknown evaluation mode\")\n",
    "\n",
    "#批量处理接口\n",
    "def aa_match_batch(\n",
    "    peptides1: Iterable,\n",
    "    peptides2: Iterable,\n",
    "    aa_dict: Dict[str, float],\n",
    "    cum_mass_threshold: float = 0.5,\n",
    "    ind_mass_threshold: float = 0.1,\n",
    "    mode: str = \"best\",\n",
    ") -> Tuple[List[Tuple[np.ndarray, bool]], int, int]:\n",
    "    \"\"\"\n",
    "    Find the matching amino acids between multiple pairs of peptide sequences.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    peptides1 : Iterable\n",
    "        The first list of peptide sequences to be compared.\n",
    "    peptides2 : Iterable\n",
    "        The second list of peptide sequences to be compared.\n",
    "    aa_dict : Dict[str, float]\n",
    "        Mapping of amino acid tokens to their mass values.\n",
    "    cum_mass_threshold : float\n",
    "        Mass threshold in Dalton to accept cumulative mass-matching amino acid\n",
    "        sequences.\n",
    "    ind_mass_threshold : float\n",
    "        Mass threshold in Dalton to accept individual mass-matching amino acids.\n",
    "    mode : {\"best\", \"forward\", \"backward\"}\n",
    "        The direction in which to find matching amino acids.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    aa_matches_batch : List[Tuple[np.ndarray, bool]]\n",
    "        For each pair of peptide sequences: (i) boolean flags indicating whether\n",
    "        each paired-up amino acid matches across both peptide sequences, (ii)\n",
    "        boolean flag to indicate whether the two peptide sequences fully match.\n",
    "    n_aa1: int\n",
    "        Total number of amino acids in the first list of peptide sequences.\n",
    "    n_aa2: int\n",
    "        Total number of amino acids in the second list of peptide sequences.\n",
    "    \"\"\"\n",
    "    aa_matches_batch, n_aa1, n_aa2 = [], 0, 0\n",
    "    for peptide1, peptide2 in zip(peptides1, peptides2):\n",
    "        # Split peptides into individual AAs if necessary.\n",
    "        if isinstance(peptide1, str):\n",
    "            peptide1 = re.split(r\"(?<=.)(?=[A-Z])\", peptide1)\n",
    "        if isinstance(peptide2, str):\n",
    "            peptide2 = re.split(r\"(?<=.)(?=[A-Z])\", peptide2)\n",
    "        n_aa1, n_aa2 = n_aa1 + len(peptide1), n_aa2 + len(peptide2)\n",
    "        aa_matches_batch.append(\n",
    "            aa_match(\n",
    "                peptide1,\n",
    "                peptide2,\n",
    "                aa_dict,\n",
    "                cum_mass_threshold,\n",
    "                ind_mass_threshold,\n",
    "                mode,\n",
    "            )\n",
    "        )\n",
    "    return aa_matches_batch, n_aa1, n_aa2\n",
    "\n",
    "#评估指标计算：基础指标\n",
    "def aa_match_metrics(\n",
    "    aa_matches_batch: List[Tuple[np.ndarray, bool]],\n",
    "    n_aa_true: int,\n",
    "    n_aa_pred: int,\n",
    ") -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate amino acid and peptide-level evaluation metrics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    aa_matches_batch : List[Tuple[np.ndarray, bool]]\n",
    "        For each pair of peptide sequences: (i) boolean flags indicating whether\n",
    "        each paired-up amino acid matches across both peptide sequences, (ii)\n",
    "        boolean flag to indicate whether the two peptide sequences fully match.\n",
    "    n_aa_true: int\n",
    "        Total number of amino acids in the true peptide sequences.\n",
    "    n_aa_pred: int\n",
    "        Total number of amino acids in the predicted peptide sequences.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    aa_precision: float\n",
    "        The number of correct AA predictions divided by the number of predicted\n",
    "        AAs.\n",
    "    aa_recall: float\n",
    "        The number of correct AA predictions divided by the number of true AAs.\n",
    "    pep_precision: float\n",
    "        The number of correct peptide predictions divided by the number of\n",
    "        peptides.\n",
    "    \"\"\"\n",
    "    n_aa_correct = sum(\n",
    "        [aa_matches[0].sum() for aa_matches in aa_matches_batch]\n",
    "    )\n",
    "    aa_precision = n_aa_correct / (n_aa_pred + 1e-8)\n",
    "    aa_recall = n_aa_correct / (n_aa_true + 1e-8)\n",
    "    pep_precision = sum([aa_matches[1] for aa_matches in aa_matches_batch]) / (\n",
    "        len(aa_matches_batch) + 1e-8\n",
    "    )\n",
    "    return float(aa_precision), float(aa_recall), float(pep_precision)\n",
    "\n",
    "#评估指标计算：阈值指标\n",
    "def aa_precision_recall(\n",
    "    aa_scores_correct: List[float],\n",
    "    aa_scores_all: List[float],\n",
    "    n_aa_total: int,\n",
    "    threshold: float,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculate amino acid level precision and recall at a given score threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    aa_scores_correct : List[float]\n",
    "        Amino acids scores for the correct amino acids predictions.\n",
    "    aa_scores_all : List[float]\n",
    "        Amino acid scores for all amino acids predictions.\n",
    "    n_aa_total : int\n",
    "        The total number of amino acids in the predicted peptide sequences.\n",
    "    threshold : float\n",
    "        The amino acid score threshold.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    aa_precision: float\n",
    "        The number of correct amino acid predictions divided by the number of\n",
    "        predicted amino acids.\n",
    "    aa_recall: float\n",
    "        The number of correct amino acid predictions divided by the total number\n",
    "        of amino acids.\n",
    "    \"\"\"\n",
    "    n_aa_correct = sum([score > threshold for score in aa_scores_correct])\n",
    "    n_aa_predicted = sum([score > threshold for score in aa_scores_all])\n",
    "    return n_aa_correct / n_aa_predicted, n_aa_correct / n_aa_total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cde76c",
   "metadata": {},
   "source": [
    "**Model**\n",
    "\n",
    "目前为MLP+Greedy Search\n",
    "\n",
    "todo:待修改为DP Search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd92e4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A de novo peptide sequencing model.\"\"\"\n",
    "\n",
    "import collections\n",
    "import heapq\n",
    "import logging\n",
    "import warnings\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n",
    "\n",
    "import depthcharge.masses\n",
    "import einops\n",
    "import torch\n",
    "import numpy as np\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from depthcharge.components import ModelMixin, PeptideDecoder, SpectrumEncoder\n",
    "from torch import nn\n",
    "\n",
    "logger = logging.getLogger(\"casanovo\")\n",
    "class GlobalPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention-based global pooling over encoder output\n",
    "    Input: [batch_size, seq_len, dim_model]\n",
    "    Output: [batch_size, dim_model]\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(dim_model, dim_model // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim_model // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_weights = self.attn(x)  # [batch_size, seq_len, 1]\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        pooled = torch.sum(x * attn_weights, dim=1)  # weighted sum over seq_len\n",
    "        return pooled\n",
    "\n",
    "\n",
    "class ResidualMLPLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single residual MLP block with dropout and LayerNorm\n",
    "    Input & Output: [batch_size, dim_model]\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(dim_model, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(hidden_dim, dim_model)\n",
    "        self.norm = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.norm(x + residual)\n",
    "\n",
    "\"\"\"Amino acid masses and other useful mass spectrometry calculations\"\"\"\n",
    "import re\n",
    "#depthcharge.masses is a module that provides mass-related utilities\n",
    "class PeptideMass:\n",
    "    \"\"\"A simple class for calculating peptide masses\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    residues: Dict or str {\"massivekb\", \"canonical\"}, optional\n",
    "        The amino acid dictionary and their masses. By default this is only\n",
    "        the 20 canonical amino acids, with cysteine carbamidomethylated. If\n",
    "        \"massivekb\", this dictionary will include the modifications found in\n",
    "        MassIVE-KB. Additionally, a dictionary can be used to specify a custom\n",
    "        collection of amino acids and masses.\n",
    "    \"\"\"\n",
    "\n",
    "    canonical = {\n",
    "        \"G\": 57.021463735,\n",
    "        \"A\": 71.037113805,\n",
    "        \"S\": 87.032028435,\n",
    "        \"P\": 97.052763875,\n",
    "        \"V\": 99.068413945,\n",
    "        \"T\": 101.047678505,\n",
    "        \"C+57.021\": 103.009184505 + 57.02146,\n",
    "        \"L\": 113.084064015,\n",
    "        \"I\": 113.084064015,\n",
    "        \"N\": 114.042927470,\n",
    "        \"D\": 115.026943065,\n",
    "        \"Q\": 128.058577540,\n",
    "        \"K\": 128.094963050,\n",
    "        \"E\": 129.042593135,\n",
    "        \"M\": 131.040484645,\n",
    "        \"H\": 137.058911875,\n",
    "        \"F\": 147.068413945,\n",
    "        # \"U\": 150.953633405,\n",
    "        \"R\": 156.101111050,\n",
    "        \"Y\": 163.063328575,\n",
    "        \"W\": 186.079312980,\n",
    "        # \"O\": 237.147726925,\n",
    "    }\n",
    "\n",
    "    # Modfications found in MassIVE-KB\n",
    "    massivekb = {\n",
    "        # N-terminal mods:\n",
    "        \"+42.011\": 42.010565,  # Acetylation\n",
    "        \"+43.006\": 43.005814,  # Carbamylation\n",
    "        \"-17.027\": -17.026549,  # NH3 loss\n",
    "        \"+43.006-17.027\": (43.006814 - 17.026549),\n",
    "        # AA mods:\n",
    "        \"M+15.995\": canonical[\"M\"] + 15.994915,  # Met Oxidation\n",
    "        \"N+0.984\": canonical[\"N\"] + 0.984016,  # Asn Deamidation\n",
    "        \"Q+0.984\": canonical[\"Q\"] + 0.984016,  # Gln Deamidation\n",
    "    }\n",
    "\n",
    "    # Constants\n",
    "    hydrogen = 1.007825035\n",
    "    oxygen = 15.99491463\n",
    "    h2o = 2 * hydrogen + oxygen\n",
    "    proton = 1.00727646688\n",
    "\n",
    "    def __init__(self, residues=\"canonical\"):\n",
    "        \"\"\"Initialize the PeptideMass object\"\"\"\n",
    "        if residues == \"canonical\":\n",
    "            self.masses = self.canonical\n",
    "        elif residues == \"massivekb\":\n",
    "            self.masses = self.canonical\n",
    "            self.masses.update(self.massivekb)\n",
    "        else:\n",
    "            self.masses = residues\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the residue dictionary\"\"\"\n",
    "        return len(self.masses)\n",
    "\n",
    "    def mass(self, seq, charge=None):\n",
    "        \"\"\"Calculate a peptide's mass or m/z.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seq : list or str\n",
    "            The peptide sequence, using tokens defined in ``self.residues``.\n",
    "        charge : int, optional\n",
    "            The charge used to compute m/z. Otherwise the neutral peptide mass\n",
    "            is calculated\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The computed mass or m/z.\n",
    "        \"\"\"\n",
    "        if isinstance(seq, str):\n",
    "            seq = re.split(r\"(?<=.)(?=[A-Z])\", seq)\n",
    "\n",
    "        calc_mass = sum([self.masses[aa] for aa in seq]) + self.h2o\n",
    "        if charge is not None:\n",
    "            calc_mass = (calc_mass / charge) + self.proton\n",
    "\n",
    "        return calc_mass\n",
    "\n",
    "class MLPDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Model:Encoder output ->Global Pooling -> Residual MLP × 4 -> Output Layer ->Output\n",
    "    Enhanced MLP Decoder for peptide de novo sequencing\n",
    "    Input: encoder_out [batch_size, seq_len, dim_model]\n",
    "    Output: amino acid logits [batch_size, max_length, n_classes]\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int = 512,\n",
    "        n_layers: int = 4,\n",
    "        dropout: float = 0.1,\n",
    "        max_length: int = 14,\n",
    "        n_classes: int = 29,\n",
    "        residues=\"canonical\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Initialize peptide mass calculator\n",
    "        self._peptide_mass = PeptideMass(residues=residues)\n",
    "        self._amino_acids = list(self._peptide_mass.masses.keys()) + [\"$\"] \n",
    "        self._idx2aa = {i + 1: aa for i, aa in enumerate(self._amino_acids)}\n",
    "        self._aa2idx = {aa: i for i, aa in self._idx2aa.items()}\n",
    "        \n",
    "        # Add vocab_size attribute\n",
    "        self.vocab_size = len(self._amino_acids)\n",
    "        #添加BatchNorm提高训练稳定性\n",
    "        self.bn = nn.BatchNorm1d(dim_model)\n",
    "        self.max_length = max_length\n",
    "        self.n_classes = n_classes\n",
    "        self.reverse = True #后期看下是True还是False\n",
    "        self._peptide_mass = PeptideMass(residues=residues)\n",
    "        self._amino_acids = list(self._peptide_mass.masses.keys()) + [\"$\"]\n",
    "        self._idx2aa = {i + 1: aa for i, aa in enumerate(self._amino_acids)}\n",
    "        self._aa2idx = {aa: i for i, aa in self._idx2aa.items()}\n",
    "        self.pooling = GlobalPooling(dim_model)\n",
    "        \n",
    "\n",
    "        self.mlp = nn.Sequential(*[\n",
    "            ResidualMLPLayer(dim_model, dim_model * 2, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(dim_model, max_length * n_classes)\n",
    "    def detokenize(self, tokens):\n",
    "        \"\"\"Convert token indices to amino acid sequence.\"\"\"\n",
    "        return [self._idx2aa[t.item()] for t in tokens if t.item() != 0]\n",
    "    \n",
    "    def forward(self, encoder_out):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder_out: [batch_size, seq_len, dim_model]\n",
    "        Returns:\n",
    "            output: [batch_size, max_length, n_classes]\n",
    "        \"\"\"\n",
    "        x = self.pooling(encoder_out)  # [batch_size, dim_model]\n",
    "        x = self.bn(x)\n",
    "        x = self.mlp(x)  # [batch_size, dim_model]\n",
    "        x = self.output_layer(x)  # [batch_size, max_length * n_classes]\n",
    "        output = x.view(x.size(0), self.max_length, self.n_classes)\n",
    "        return output\n",
    "\n",
    "class Spec2Pep(pl.LightningModule, ModelMixin):\n",
    "    \"\"\"\n",
    "    A Transformer model for de novo peptide sequencing.\n",
    "\n",
    "    Use this model in conjunction with a pytorch-lightning Trainer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim_model : int\n",
    "        The latent dimensionality used by the transformer model.\n",
    "    n_head : int\n",
    "        The number of attention heads in each layer. ``dim_model`` must be\n",
    "        divisible by ``n_head``.\n",
    "    dim_feedforward : int\n",
    "        The dimensionality of the fully connected layers in the transformer\n",
    "        model.\n",
    "    n_layers : int\n",
    "        The number of transformer layers.\n",
    "    dropout : float\n",
    "        The dropout probability for all layers.\n",
    "    dim_intensity : Optional[int]\n",
    "        The number of features to use for encoding peak intensity. The remaining\n",
    "        (``dim_model - dim_intensity``) are reserved for encoding the m/z value.\n",
    "        If ``None``, the intensity will be projected up to ``dim_model`` using a\n",
    "        linear layer, then summed with the m/z encoding for each peak.\n",
    "    max_length : int\n",
    "        The maximum peptide length to decode.\n",
    "    residues : Union[Dict[str, float], str]\n",
    "        The amino acid dictionary and their masses. By default (\"canonical) this\n",
    "        is only the 20 canonical amino acids, with cysteine carbamidomethylated.\n",
    "        If \"massivekb\", this dictionary will include the modifications found in\n",
    "        MassIVE-KB. Additionally, a dictionary can be used to specify a custom\n",
    "        collection of amino acids and masses.\n",
    "    max_charge : int\n",
    "        The maximum precursor charge to consider.\n",
    "    precursor_mass_tol : float, optional\n",
    "        The maximum allowable precursor mass tolerance (in ppm) for correct\n",
    "        predictions.\n",
    "    isotope_error_range : Tuple[int, int]\n",
    "        Take into account the error introduced by choosing a non-monoisotopic\n",
    "        peak for fragmentation by not penalizing predicted precursor m/z's that\n",
    "        fit the specified isotope error:\n",
    "        `abs(calc_mz - (precursor_mz - isotope * 1.00335 / precursor_charge))\n",
    "        < precursor_mass_tol`\n",
    "    min_peptide_len : int\n",
    "        The minimum length of predicted peptides.\n",
    "    n_beams : int\n",
    "        Number of beams used during beam search decoding.\n",
    "    top_match : int\n",
    "        Number of PSMs to return for each spectrum.\n",
    "    n_log : int\n",
    "        The number of epochs to wait between logging messages.\n",
    "    tb_summarywriter : Optional[str]\n",
    "        Folder path to record performance metrics during training. If ``None``,\n",
    "        don't use a ``SummaryWriter``.\n",
    "    train_label_smoothing : float\n",
    "        Smoothing factor when calculating the training loss.\n",
    "    warmup_iters : int\n",
    "        The number of iterations for the linear warm-up of the learning rate.\n",
    "    cosine_schedule_period_iters : int\n",
    "        The number of iterations for the cosine half period of the learning rate.\n",
    "    out_writer : Optional[str]\n",
    "        The output writer for the prediction results.\n",
    "    calculate_precision : bool\n",
    "        Calculate the validation set precision during training.\n",
    "        This is expensive.\n",
    "    **kwargs : Dict\n",
    "        Additional keyword arguments passed to the Adam optimizer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int = 512,\n",
    "        n_head: int = 8,\n",
    "        dim_feedforward: int = 1024,\n",
    "        n_layers: int = 9,\n",
    "        dropout: float = 0.0,\n",
    "        dim_intensity: Optional[int] = None,\n",
    "        max_length: int = 14,\n",
    "        residues: Union[Dict[str, float], str] = \"canonical\",\n",
    "        max_charge: int = 5,\n",
    "        precursor_mass_tol: float = 50,\n",
    "        isotope_error_range: Tuple[int, int] = (0, 1),\n",
    "        min_peptide_len: int = 6,\n",
    "        n_beams: int = 1,\n",
    "        top_match: int = 1,\n",
    "        n_log: int = 10,\n",
    "        tb_summarywriter: Optional[\n",
    "            torch.utils.tensorboard.SummaryWriter\n",
    "        ] = None,\n",
    "        train_label_smoothing: float = 0.01,\n",
    "        warmup_iters: int = 100_000,\n",
    "        cosine_schedule_period_iters: int = 600_000,\n",
    "        out_writer: Optional[MztabWriter] = None,\n",
    "        calculate_precision: bool = False,\n",
    "        **kwargs: Dict,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() #把所有传入的参数记录到Lighning的hparms中，方便后续checkpoint/日志查看\n",
    "        \n",
    "        # Initialize peptide mass calculator first\n",
    "        self.residues = residues\n",
    "        self.peptide_mass_calculator = depthcharge.masses.PeptideMass(\n",
    "            self.residues\n",
    "        )\n",
    "        # Build the model.\n",
    "        self.encoder = SpectrumEncoder(\n",
    "            dim_model=dim_model,\n",
    "            n_head=n_head,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            n_layers=n_layers,\n",
    "            dropout=dropout,\n",
    "            dim_intensity=dim_intensity,\n",
    "        )\n",
    "        self.decoder = MLPDecoder(\n",
    "            dim_model=dim_model,\n",
    "            n_layers=n_layers,\n",
    "            dropout=dropout,\n",
    "            max_length=max_length,\n",
    "            n_classes=len(self.peptide_mass_calculator.masses) + 1, # +1 for stop token\n",
    "            residues = residues,\n",
    "        )\n",
    "        # self.decoder = PeptideDecoder(\n",
    "        #     dim_model=dim_model,\n",
    "        #     n_head=n_head,\n",
    "        #     dim_feedforward=dim_feedforward,\n",
    "        #     n_layers=n_layers,\n",
    "        #     dropout=dropout,\n",
    "        #     residues=residues,\n",
    "        #     max_charge=max_charge,\n",
    "        # )\n",
    "        self.softmax = torch.nn.Softmax(2)\n",
    "        self.celoss = torch.nn.CrossEntropyLoss(\n",
    "            ignore_index=0, label_smoothing=train_label_smoothing\n",
    "        )\n",
    "        self.val_celoss = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "        # Optimizer settings.\n",
    "        self.warmup_iters = warmup_iters\n",
    "        self.cosine_schedule_period_iters = cosine_schedule_period_iters\n",
    "        # `kwargs` will contain additional arguments as well as unrecognized\n",
    "        # arguments, including deprecated ones. Remove the deprecated ones.\n",
    "        for k in Config._config_deprecated:\n",
    "            kwargs.pop(k, None)\n",
    "            warnings.warn(\n",
    "                f\"Deprecated hyperparameter '{k}' removed from the model.\",\n",
    "                DeprecationWarning,\n",
    "            )\n",
    "        self.opt_kwargs = kwargs\n",
    "\n",
    "        # Data properties.\n",
    "        self.max_length = max_length\n",
    "        self.precursor_mass_tol = precursor_mass_tol\n",
    "        self.isotope_error_range = isotope_error_range\n",
    "        self.min_peptide_len = min_peptide_len\n",
    "        self.n_beams = n_beams\n",
    "        self.top_match = top_match\n",
    "        \n",
    "        self.stop_token = self.decoder._aa2idx[\"$\"]\n",
    "\n",
    "        # Logging.\n",
    "        self.calculate_precision = calculate_precision\n",
    "        self.n_log = n_log\n",
    "        self._history = []\n",
    "        if tb_summarywriter is not None:\n",
    "            self.tb_summarywriter = SummaryWriter(tb_summarywriter)\n",
    "        else:\n",
    "            self.tb_summarywriter = tb_summarywriter\n",
    "\n",
    "        # Output writer during predicting.\n",
    "        self.out_writer = out_writer\n",
    "\n",
    "    def forward(\n",
    "        self, spectra: torch.Tensor, precursors: torch.Tensor\n",
    "    ) -> List[List[Tuple[float, np.ndarray, str]]]:\n",
    "        \"\"\"\n",
    "        Predict peptide sequences for a batch of MS/MS spectra.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        spectra : torch.Tensor of shape (n_spectra, n_peaks, 2)\n",
    "            The spectra for which to predict peptide sequences.\n",
    "            Axis 0 represents an MS/MS spectrum, axis 1 contains the peaks in\n",
    "            the MS/MS spectrum, and axis 2 is essentially a 2-tuple specifying\n",
    "            the m/z-intensity pair for each peak. These should be zero-padded,\n",
    "            such that all the spectra in the batch are the same length.\n",
    "        precursors : torch.Tensor of size (n_spectra, 3)\n",
    "            The measured precursor mass (axis 0), precursor charge (axis 1), and\n",
    "            precursor m/z (axis 2) of each MS/MS spectrum.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pred_peptides : List[List[Tuple[float, np.ndarray, str]]]\n",
    "            For each spectrum, a list with the top peptide predictions. A\n",
    "            peptide predictions consists of a tuple with the peptide score,\n",
    "            the amino acid scores, and the predicted peptide sequence.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.dp_search_decode(\n",
    "            spectra.to(self.encoder.device),\n",
    "            precursors.to(self.decoder.device),\n",
    "        )\n",
    "\n",
    "    def dp_search_decode(self,spectra:torch.Tensor,precursors:torch.Tensor):\n",
    "        \"\"\"Dp non-autoregressive decoding.\"\"\"\n",
    "        memories, mem_masks = self.encoder(spectra)\n",
    "\n",
    "        #获取每个位置的预测概率\n",
    "        logits = self.decoder(memories)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        #对每个位置取最大概率的token\n",
    "        pred_tokens = torch.argmax(probs, dim=-1)\n",
    "\n",
    "        #把预测tokens转换为氨基酸序列\n",
    "        predictions = []\n",
    "        for i in range(len(spectra)):\n",
    "            tokens = pred_tokens[i].tolist()\n",
    "            peptide = []\n",
    "            aa_scores = []\n",
    "\n",
    "            #找到第一个stop token或者序列结束\n",
    "            for j, t in enumerate(tokens):\n",
    "                if t == self.stop_token:\n",
    "                    break\n",
    "                if t == 0: #padding \n",
    "                    break #遇0则不再生成后面的序列\n",
    "                peptide.append(self.decoder._idx2aa[t])\n",
    "                aa_scores.append(probs[i, j, t].item())\n",
    "            if len(peptide) > 0:\n",
    "                peptide_score = np.mean(aa_scores)\n",
    "                predictions.append(\n",
    "                    (peptide_score, np.array(aa_scores), \"\".join(peptide))\n",
    "                )\n",
    "            else:\n",
    "                predictions.append([])\n",
    "        return predictions\n",
    "\n",
    "\n",
    "\n",
    "    def beam_search_decode(\n",
    "        self, spectra: torch.Tensor, precursors: torch.Tensor\n",
    "    ) -> List[List[Tuple[float, np.ndarray, str]]]:\n",
    "        \"\"\"\n",
    "        Beam search decoding of the spectrum predictions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        spectra : torch.Tensor of shape (n_spectra, n_peaks, 2)\n",
    "            The spectra for which to predict peptide sequences.\n",
    "            Axis 0 represents an MS/MS spectrum, axis 1 contains the peaks in\n",
    "            the MS/MS spectrum, and axis 2 is essentially a 2-tuple specifying\n",
    "            the m/z-intensity pair for each peak. These should be zero-padded,\n",
    "            such that all the spectra in the batch are the same length.\n",
    "        precursors : torch.Tensor of size (n_spectra, 3)\n",
    "            The measured precursor mass (axis 0), precursor charge (axis 1), and\n",
    "            precursor m/z (axis 2) of each MS/MS spectrum.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pred_peptides : List[List[Tuple[float, np.ndarray, str]]]\n",
    "            For each spectrum, a list with the top peptide prediction(s). A\n",
    "            peptide predictions consists of a tuple with the peptide score,\n",
    "            the amino acid scores, and the predicted peptide sequence.\n",
    "        \"\"\"\n",
    "        memories, mem_masks = self.encoder(spectra)\n",
    "\n",
    "        # Sizes.\n",
    "        batch = spectra.shape[0]  # B\n",
    "        length = self.max_length + 1  # L\n",
    "        vocab = self.decoder.vocab_size + 1  # V\n",
    "        beam = self.n_beams  # S\n",
    "\n",
    "        # Initialize scores and tokens.\n",
    "        scores = torch.full(\n",
    "            size=(batch, length, vocab, beam), fill_value=torch.nan\n",
    "        )\n",
    "        scores = scores.type_as(spectra)\n",
    "        tokens = torch.zeros(batch, length, beam, dtype=torch.int64)\n",
    "        tokens = tokens.to(self.encoder.device)\n",
    "\n",
    "        # Create cache for decoded beams.\n",
    "        pred_cache = collections.OrderedDict((i, []) for i in range(batch))\n",
    "\n",
    "        # Get the first prediction.\n",
    "        pred, _ = self.decoder(None, precursors, memories, mem_masks)\n",
    "        tokens[:, 0, :] = torch.topk(pred[:, 0, :], beam, dim=1)[1] #后期把beam改为2观察一下\n",
    "        scores[:, :1, :, :] = einops.repeat(pred, \"B L V -> B L V S\", S=beam)\n",
    "\n",
    "        # Make all tensors the right shape for decoding.\n",
    "        precursors = einops.repeat(precursors, \"B L -> (B S) L\", S=beam)\n",
    "        mem_masks = einops.repeat(mem_masks, \"B L -> (B S) L\", S=beam)\n",
    "        memories = einops.repeat(memories, \"B L V -> (B S) L V\", S=beam)\n",
    "        tokens = einops.rearrange(tokens, \"B L S -> (B S) L\")\n",
    "        scores = einops.rearrange(scores, \"B L V S -> (B S) L V\")\n",
    "\n",
    "        # The main decoding loop.\n",
    "        for step in range(0, self.max_length):\n",
    "            # Terminate beams exceeding the precursor m/z tolerance and track\n",
    "            # all finished beams (either terminated or stop token predicted).\n",
    "            (\n",
    "                finished_beams,\n",
    "                beam_fits_precursor,\n",
    "                discarded_beams,\n",
    "            ) = self._finish_beams(tokens, precursors, step)\n",
    "            # Cache peptide predictions from the finished beams (but not the\n",
    "            # discarded beams).\n",
    "            self._cache_finished_beams(\n",
    "                tokens,\n",
    "                scores,\n",
    "                step,\n",
    "                finished_beams & ~discarded_beams,\n",
    "                beam_fits_precursor,\n",
    "                pred_cache,\n",
    "            )\n",
    "\n",
    "            # Stop decoding when all current beams have been finished.\n",
    "            # Continue with beams that have not been finished and not discarded.\n",
    "            finished_beams |= discarded_beams\n",
    "            if finished_beams.all():\n",
    "                break\n",
    "            # Update the scores.\n",
    "            scores[~finished_beams, : step + 2, :], _ = self.decoder(\n",
    "                tokens[~finished_beams, : step + 1],\n",
    "                precursors[~finished_beams, :],\n",
    "                memories[~finished_beams, :, :],\n",
    "                mem_masks[~finished_beams, :],\n",
    "            )\n",
    "            # Find the top-k beams with the highest scores and continue decoding\n",
    "            # those.\n",
    "            tokens, scores = self._get_topk_beams(\n",
    "                tokens, scores, finished_beams, batch, step + 1\n",
    "            )\n",
    "\n",
    "        # Return the peptide with the highest confidence score, within the\n",
    "        # precursor m/z tolerance if possible.\n",
    "        return list(self._get_top_peptide(pred_cache))\n",
    "\n",
    "    def _finish_beams(\n",
    "        self,\n",
    "        tokens: torch.Tensor,\n",
    "        precursors: torch.Tensor,\n",
    "        step: int,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Track all beams that have been finished, either by predicting the stop\n",
    "        token or because they were terminated due to exceeding the precursor\n",
    "        m/z tolerance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens : torch.Tensor of shape (n_spectra * n_beams, max_length)\n",
    "            Predicted amino acid tokens for all beams and all spectra.\n",
    "         scores : torch.Tensor of shape\n",
    "         (n_spectra *  n_beams, max_length, n_amino_acids)\n",
    "            Scores for the predicted amino acid tokens for all beams and all\n",
    "            spectra.\n",
    "        step : int\n",
    "            Index of the current decoding step.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        finished_beams : torch.Tensor of shape (n_spectra * n_beams)\n",
    "            Boolean tensor indicating whether the current beams have been\n",
    "            finished.\n",
    "        beam_fits_precursor: torch.Tensor of shape (n_spectra * n_beams)\n",
    "            Boolean tensor indicating if current beams are within precursor m/z\n",
    "            tolerance.\n",
    "        discarded_beams : torch.Tensor of shape (n_spectra * n_beams)\n",
    "            Boolean tensor indicating whether the current beams should be\n",
    "            discarded (e.g. because they were predicted to end but violate the\n",
    "            minimum peptide length).\n",
    "        \"\"\"\n",
    "        # Check for tokens with a negative mass (i.e. neutral loss).\n",
    "        aa_neg_mass = [None]\n",
    "        for aa, mass in self.peptide_mass_calculator.masses.items():\n",
    "            if mass < 0:\n",
    "                aa_neg_mass.append(aa)\n",
    "        # Find N-terminal residues.\n",
    "        n_term = torch.Tensor(\n",
    "            [\n",
    "                self.decoder._aa2idx[aa]\n",
    "                for aa in self.peptide_mass_calculator.masses\n",
    "                if aa.startswith((\"+\", \"-\"))\n",
    "            ]\n",
    "        ).to(self.decoder.device)\n",
    "\n",
    "        beam_fits_precursor = torch.zeros(\n",
    "            tokens.shape[0], dtype=torch.bool\n",
    "        ).to(self.encoder.device)\n",
    "        # Beams with a stop token predicted in the current step can be finished.\n",
    "        finished_beams = torch.zeros(tokens.shape[0], dtype=torch.bool).to(\n",
    "            self.encoder.device\n",
    "        )\n",
    "        ends_stop_token = tokens[:, step] == self.stop_token\n",
    "        finished_beams[ends_stop_token] = True\n",
    "        # Beams with a dummy token predicted in the current step can be\n",
    "        # discarded.\n",
    "        discarded_beams = torch.zeros(tokens.shape[0], dtype=torch.bool).to(\n",
    "            self.encoder.device\n",
    "        )\n",
    "        discarded_beams[tokens[:, step] == 0] = True\n",
    "        # Discard beams with invalid modification combinations (i.e. N-terminal\n",
    "        # modifications occur multiple times or in internal positions).\n",
    "        if step > 1:  # Only relevant for longer predictions.\n",
    "            dim0 = torch.arange(tokens.shape[0])\n",
    "            final_pos = torch.full((ends_stop_token.shape[0],), step)\n",
    "            final_pos[ends_stop_token] = step - 1\n",
    "            # Multiple N-terminal modifications.\n",
    "            multiple_mods = torch.isin(\n",
    "                tokens[dim0, final_pos], n_term\n",
    "            ) & torch.isin(tokens[dim0, final_pos - 1], n_term)\n",
    "            # N-terminal modifications occur at an internal position.\n",
    "            # Broadcasting trick to create a two-dimensional mask.\n",
    "            mask = (final_pos - 1)[:, None] >= torch.arange(tokens.shape[1])\n",
    "            internal_mods = torch.isin(\n",
    "                torch.where(mask.to(self.encoder.device), tokens, 0), n_term\n",
    "            ).any(dim=1)\n",
    "            discarded_beams[multiple_mods | internal_mods] = True\n",
    "\n",
    "        # Check which beams should be terminated or discarded based on the\n",
    "        # predicted peptide.\n",
    "        for i in range(len(finished_beams)):\n",
    "            # Skip already discarded beams.\n",
    "            if discarded_beams[i]:\n",
    "                continue\n",
    "            pred_tokens = tokens[i][: step + 1]\n",
    "            peptide_len = len(pred_tokens)\n",
    "            peptide = self.decoder.detokenize(pred_tokens)\n",
    "            # Omit stop token.\n",
    "            if self.decoder.reverse and peptide[0] == \"$\":\n",
    "                peptide = peptide[1:]\n",
    "                peptide_len -= 1\n",
    "            elif not self.decoder.reverse and peptide[-1] == \"$\":\n",
    "                peptide = peptide[:-1]\n",
    "                peptide_len -= 1\n",
    "            # Discard beams that were predicted to end but don't fit the minimum\n",
    "            # peptide length.\n",
    "            if finished_beams[i] and peptide_len < self.min_peptide_len:\n",
    "                discarded_beams[i] = True\n",
    "                continue\n",
    "            # Terminate the beam if it has not been finished by the model but\n",
    "            # the peptide mass exceeds the precursor m/z to an extent that it\n",
    "            # cannot be corrected anymore by a subsequently predicted AA with\n",
    "            # negative mass.\n",
    "            precursor_charge = precursors[i, 1]\n",
    "            precursor_mz = precursors[i, 2]\n",
    "            matches_precursor_mz = exceeds_precursor_mz = False\n",
    "            for aa in [None] if finished_beams[i] else aa_neg_mass:\n",
    "                if aa is None:\n",
    "                    calc_peptide = peptide\n",
    "                else:\n",
    "                    calc_peptide = peptide.copy()\n",
    "                    calc_peptide.append(aa)\n",
    "                try:\n",
    "                    calc_mz = self.peptide_mass_calculator.mass(\n",
    "                        seq=calc_peptide, charge=precursor_charge\n",
    "                    )\n",
    "                    delta_mass_ppm = [\n",
    "                        _calc_mass_error(\n",
    "                            calc_mz,\n",
    "                            precursor_mz,\n",
    "                            precursor_charge,\n",
    "                            isotope,\n",
    "                        )\n",
    "                        for isotope in range(\n",
    "                            self.isotope_error_range[0],\n",
    "                            self.isotope_error_range[1] + 1,\n",
    "                        )\n",
    "                    ]\n",
    "                    # Terminate the beam if the calculated m/z for the predicted\n",
    "                    # peptide (without potential additional AAs with negative\n",
    "                    # mass) is within the precursor m/z tolerance.\n",
    "                    matches_precursor_mz = aa is None and any(\n",
    "                        abs(d) < self.precursor_mass_tol\n",
    "                        for d in delta_mass_ppm\n",
    "                    )\n",
    "                    # Terminate the beam if the calculated m/z exceeds the\n",
    "                    # precursor m/z + tolerance and hasn't been corrected by a\n",
    "                    # subsequently predicted AA with negative mass.\n",
    "                    if matches_precursor_mz:\n",
    "                        exceeds_precursor_mz = False\n",
    "                    else:\n",
    "                        exceeds_precursor_mz = all(\n",
    "                            d > self.precursor_mass_tol for d in delta_mass_ppm\n",
    "                        )\n",
    "                        exceeds_precursor_mz = (\n",
    "                            finished_beams[i] or aa is not None\n",
    "                        ) and exceeds_precursor_mz\n",
    "                    if matches_precursor_mz or exceeds_precursor_mz:\n",
    "                        break\n",
    "                except KeyError:\n",
    "                    matches_precursor_mz = exceeds_precursor_mz = False\n",
    "            # Finish beams that fit or exceed the precursor m/z.\n",
    "            # Don't finish beams that don't include a stop token if they don't\n",
    "            # exceed the precursor m/z tolerance yet.\n",
    "            if finished_beams[i]:\n",
    "                beam_fits_precursor[i] = matches_precursor_mz\n",
    "            elif exceeds_precursor_mz:\n",
    "                finished_beams[i] = True\n",
    "                beam_fits_precursor[i] = matches_precursor_mz\n",
    "        return finished_beams, beam_fits_precursor, discarded_beams\n",
    "\n",
    "    def _cache_finished_beams(\n",
    "        self,\n",
    "        tokens: torch.Tensor,\n",
    "        scores: torch.Tensor,\n",
    "        step: int,\n",
    "        beams_to_cache: torch.Tensor,\n",
    "        beam_fits_precursor: torch.Tensor,\n",
    "        pred_cache: Dict[\n",
    "            int, List[Tuple[float, float, np.ndarray, torch.Tensor]]\n",
    "        ],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Cache terminated beams.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens : torch.Tensor of shape (n_spectra * n_beams, max_length)\n",
    "            Predicted amino acid tokens for all beams and all spectra.\n",
    "         scores : torch.Tensor of shape\n",
    "         (n_spectra *  n_beams, max_length, n_amino_acids)\n",
    "            Scores for the predicted amino acid tokens for all beams and all\n",
    "            spectra.\n",
    "        step : int\n",
    "            Index of the current decoding step.\n",
    "        beams_to_cache : torch.Tensor of shape (n_spectra * n_beams)\n",
    "            Boolean tensor indicating whether the current beams are ready for\n",
    "            caching.\n",
    "        beam_fits_precursor: torch.Tensor of shape (n_spectra * n_beams)\n",
    "            Boolean tensor indicating whether the beams are within the\n",
    "            precursor m/z tolerance.\n",
    "        pred_cache : Dict[\n",
    "                int, List[Tuple[float, float, np.ndarray, torch.Tensor]]\n",
    "        ]\n",
    "            Priority queue with finished beams for each spectrum, ordered by\n",
    "            peptide score. For each finished beam, a tuple with the (negated)\n",
    "            peptide score, a random tie-breaking float, the amino acid-level\n",
    "            scores, and the predicted tokens is stored.\n",
    "        \"\"\"\n",
    "        for i in range(len(beams_to_cache)):\n",
    "            if not beams_to_cache[i]:\n",
    "                continue\n",
    "            # Find the starting index of the spectrum.\n",
    "            spec_idx = i // self.n_beams\n",
    "            # FIXME: The next 3 lines are very similar as what's done in\n",
    "            #  _finish_beams. Avoid code duplication?\n",
    "            pred_tokens = tokens[i][: step + 1]\n",
    "            # Omit the stop token from the peptide sequence (if predicted).\n",
    "            has_stop_token = pred_tokens[-1] == self.stop_token\n",
    "            pred_peptide = pred_tokens[:-1] if has_stop_token else pred_tokens\n",
    "            # Don't cache this peptide if it was already predicted previously.\n",
    "            if any(\n",
    "                torch.equal(pred_cached[-1], pred_peptide)\n",
    "                for pred_cached in pred_cache[spec_idx]\n",
    "            ):\n",
    "                # TODO: Add duplicate predictions with their highest score.\n",
    "                continue\n",
    "            smx = self.softmax(scores[i : i + 1, : step + 1, :])\n",
    "            aa_scores = smx[0, range(len(pred_tokens)), pred_tokens].tolist()\n",
    "            # Add an explicit score 0 for the missing stop token in case this\n",
    "            # was not predicted (i.e. early stopping).\n",
    "            if not has_stop_token:\n",
    "                aa_scores.append(0)\n",
    "            aa_scores = np.asarray(aa_scores)\n",
    "            # Calculate the updated amino acid-level and the peptide scores.\n",
    "            aa_scores, peptide_score = _aa_pep_score(\n",
    "                aa_scores, beam_fits_precursor[i]\n",
    "            )\n",
    "            # Omit the stop token from the amino acid-level scores.\n",
    "            aa_scores = aa_scores[:-1]\n",
    "            # Add the prediction to the cache (minimum priority queue, maximum\n",
    "            # the number of beams elements).\n",
    "            if len(pred_cache[spec_idx]) < self.n_beams:\n",
    "                heapadd = heapq.heappush\n",
    "            else:\n",
    "                heapadd = heapq.heappushpop\n",
    "            heapadd(\n",
    "                pred_cache[spec_idx],\n",
    "                (\n",
    "                    peptide_score,\n",
    "                    np.random.random_sample(),\n",
    "                    aa_scores,\n",
    "                    torch.clone(pred_peptide),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def _get_topk_beams(\n",
    "        self,\n",
    "        tokens: torch.tensor,\n",
    "        scores: torch.tensor,\n",
    "        finished_beams: torch.tensor,\n",
    "        batch: int,\n",
    "        step: int,\n",
    "    ) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        \"\"\"\n",
    "        Find the top-k beams with the highest scores and continue decoding\n",
    "        those.\n",
    "\n",
    "        Stop decoding for beams that have been finished.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens : torch.Tensor of shape (n_spectra * n_beams, max_length)\n",
    "            Predicted amino acid tokens for all beams and all spectra.\n",
    "         scores : torch.Tensor of shape\n",
    "         (n_spectra *  n_beams, max_length, n_amino_acids)\n",
    "            Scores for the predicted amino acid tokens for all beams and all\n",
    "            spectra.\n",
    "        finished_beams : torch.Tensor of shape (n_spectra * n_beams)\n",
    "            Boolean tensor indicating whether the current beams are ready for\n",
    "            caching.\n",
    "        batch: int\n",
    "            Number of spectra in the batch.\n",
    "        step : int\n",
    "            Index of the next decoding step.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tokens : torch.Tensor of shape (n_spectra * n_beams, max_length)\n",
    "            Predicted amino acid tokens for all beams and all spectra.\n",
    "         scores : torch.Tensor of shape\n",
    "         (n_spectra *  n_beams, max_length, n_amino_acids)\n",
    "            Scores for the predicted amino acid tokens for all beams and all\n",
    "            spectra.\n",
    "        \"\"\"\n",
    "        beam = self.n_beams  # S\n",
    "        vocab = self.decoder.vocab_size + 1  # V\n",
    "\n",
    "        # Reshape to group by spectrum (B for \"batch\").\n",
    "        tokens = einops.rearrange(tokens, \"(B S) L -> B L S\", S=beam)\n",
    "        scores = einops.rearrange(scores, \"(B S) L V -> B L V S\", S=beam)\n",
    "\n",
    "        # Get the previous tokens and scores.\n",
    "        prev_tokens = einops.repeat(\n",
    "            tokens[:, :step, :], \"B L S -> B L V S\", V=vocab\n",
    "        )\n",
    "        prev_scores = torch.gather(\n",
    "            scores[:, :step, :, :], dim=2, index=prev_tokens\n",
    "        )\n",
    "        prev_scores = einops.repeat(\n",
    "            prev_scores[:, :, 0, :], \"B L S -> B L (V S)\", V=vocab\n",
    "        )\n",
    "\n",
    "        # Get the scores for all possible beams at this step.\n",
    "        step_scores = torch.zeros(batch, step + 1, beam * vocab).type_as(\n",
    "            scores\n",
    "        )\n",
    "        step_scores[:, :step, :] = prev_scores\n",
    "        step_scores[:, step, :] = einops.rearrange(\n",
    "            scores[:, step, :, :], \"B V S -> B (V S)\"\n",
    "        )\n",
    "\n",
    "        # Find all still active beams by masking out terminated beams.\n",
    "        active_mask = (\n",
    "            ~finished_beams.reshape(batch, beam).repeat(1, vocab)\n",
    "        ).float()\n",
    "        # Mask out the index '0', i.e. padding token, by default.\n",
    "        # FIXME: Set this to a very small, yet non-zero value, to only\n",
    "        # get padding after stop token.\n",
    "        active_mask[:, :beam] = 1e-8\n",
    "\n",
    "        # Figure out the top K decodings.\n",
    "        _, top_idx = torch.topk(step_scores.nanmean(dim=1) * active_mask, beam)\n",
    "        v_idx, s_idx = np.unravel_index(top_idx.cpu(), (vocab, beam))\n",
    "        s_idx = einops.rearrange(s_idx, \"B S -> (B S)\")\n",
    "        b_idx = einops.repeat(torch.arange(batch), \"B -> (B S)\", S=beam)\n",
    "\n",
    "        # Record the top K decodings.\n",
    "        tokens[:, :step, :] = einops.rearrange(\n",
    "            prev_tokens[b_idx, :, 0, s_idx], \"(B S) L -> B L S\", S=beam\n",
    "        )\n",
    "        tokens[:, step, :] = torch.tensor(v_idx)\n",
    "        scores[:, : step + 1, :, :] = einops.rearrange(\n",
    "            scores[b_idx, : step + 1, :, s_idx], \"(B S) L V -> B L V S\", S=beam\n",
    "        )\n",
    "        scores = einops.rearrange(scores, \"B L V S -> (B S) L V\")\n",
    "        tokens = einops.rearrange(tokens, \"B L S -> (B S) L\")\n",
    "        return tokens, scores\n",
    "\n",
    "    def _get_top_peptide(\n",
    "        self,\n",
    "        pred_cache: Dict[\n",
    "            int, List[Tuple[float, float, np.ndarray, torch.Tensor]]\n",
    "        ],\n",
    "    ) -> Iterable[List[Tuple[float, np.ndarray, str]]]:\n",
    "        \"\"\"\n",
    "        Return the peptide with the highest confidence score for each spectrum.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pred_cache : Dict[\n",
    "                int, List[Tuple[float, float, np.ndarray, torch.Tensor]]\n",
    "        ]\n",
    "            Priority queue with finished beams for each spectrum, ordered by\n",
    "            peptide score. For each finished beam, a tuple with the peptide\n",
    "            score, a random tie-breaking float, the amino acid-level scores,\n",
    "            and the predicted tokens is stored.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pred_peptides : Iterable[List[Tuple[float, np.ndarray, str]]]\n",
    "            For each spectrum, a list with the top peptide prediction(s). A\n",
    "            peptide predictions consists of a tuple with the peptide score,\n",
    "            the amino acid scores, and the predicted peptide sequence.\n",
    "        \"\"\"\n",
    "        for peptides in pred_cache.values():\n",
    "            if len(peptides) > 0:\n",
    "                yield [\n",
    "                    (\n",
    "                        pep_score,\n",
    "                        aa_scores[::-1] if self.decoder.reverse else aa_scores,\n",
    "                        \"\".join(self.decoder.detokenize(pred_tokens)),\n",
    "                    )\n",
    "                    for pep_score, _, aa_scores, pred_tokens in heapq.nlargest(\n",
    "                        self.top_match, peptides\n",
    "                    )\n",
    "                ]\n",
    "            else:\n",
    "                yield []\n",
    "    def _forward_step(\n",
    "        self,\n",
    "        spectra: torch.Tensor,\n",
    "        precursors: torch.Tensor,\n",
    "        sequences: List[str],\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward step for training.\"\"\"\n",
    "        #暂定，看看是否需要修改\n",
    "        memories, mem_masks = self.encoder(spectra)\n",
    "\n",
    "        #MLPDecoder直接输出所有位置的logits\n",
    "        logits = self.decoder(memories)\n",
    "        \n",
    "        #处理目标序列\n",
    "        target_tokens = torch.zeros(len(sequences), self.max_length).long()\n",
    "        for i, seq in enumerate(sequences):\n",
    "            tokens = [self.decoder._aa2idx.get(aa, 0) for aa in seq]\n",
    "            target_tokens[i,:len(tokens)] = torch.tensor(tokens)\n",
    "        \n",
    "        return logits, target_tokens.to(logits.device)\n",
    "\n",
    "\n",
    "\n",
    "    # def _forward_step(\n",
    "    #     self,\n",
    "    #     spectra: torch.Tensor,\n",
    "    #     precursors: torch.Tensor,\n",
    "    #     sequences: List[str],\n",
    "    # ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    #     \"\"\"\n",
    "    #     The forward learning step.\n",
    "\n",
    "    #     Parameters\n",
    "    #     ----------\n",
    "    #     spectra : torch.Tensor of shape (n_spectra, n_peaks, 2)\n",
    "    #         The spectra for which to predict peptide sequences.\n",
    "    #         Axis 0 represents an MS/MS spectrum, axis 1 contains the peaks in\n",
    "    #         the MS/MS spectrum, and axis 2 is essentially a 2-tuple specifying\n",
    "    #         the m/z-intensity pair for each peak. These should be zero-padded,\n",
    "    #         such that all the spectra in the batch are the same length.\n",
    "    #     precursors : torch.Tensor of size (n_spectra, 3)\n",
    "    #         The measured precursor mass (axis 0), precursor charge (axis 1), and\n",
    "    #         precursor m/z (axis 2) of each MS/MS spectrum.\n",
    "    #     sequences : List[str] of length n_spectra\n",
    "    #         The partial peptide sequences to predict.\n",
    "\n",
    "    #     Returns\n",
    "    #     -------\n",
    "    #     scores : torch.Tensor of shape (n_spectra, length, n_amino_acids)\n",
    "    #         The individual amino acid scores for each prediction.\n",
    "    #     tokens : torch.Tensor of shape (n_spectra, length)\n",
    "    #         The predicted tokens for each spectrum.\n",
    "    #     \"\"\"\n",
    "    #     return self.decoder(sequences, precursors, *self.encoder(spectra))\n",
    "\n",
    "    def training_step(\n",
    "        self,\n",
    "        batch: Tuple[torch.Tensor, torch.Tensor, List[str]],\n",
    "        *args,\n",
    "        mode: str = \"train\",\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        A single training step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : Tuple[torch.Tensor, torch.Tensor, List[str]]\n",
    "            A batch of (i) MS/MS spectra, (ii) precursor information, (iii)\n",
    "            peptide sequences as torch Tensors.\n",
    "        mode : str\n",
    "            Logging key to describe the current stage.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The loss of the training step.\n",
    "        \"\"\"\n",
    "        pred, truth = self._forward_step(*batch)\n",
    "        \n",
    "        # 获取实际的vocabulary size\n",
    "        vocab_size = self.decoder.vocab_size\n",
    "        batch_size = pred.shape[0]\n",
    "        seq_len = pred.shape[1]\n",
    "    \n",
    "        # 正确的reshape操作\n",
    "        pred = pred.view(-1, vocab_size)  # [batch_size * seq_len, vocab_size]\n",
    "        truth = truth.view(-1)  # [batch_size * seq_len, vocab_size] \n",
    "\n",
    "        if mode == \"train\":\n",
    "            loss = self.celoss(pred, truth)\n",
    "        else:\n",
    "            loss = self.val_celoss(pred, truth)\n",
    "        self.log(\n",
    "            f\"{mode}_CELoss\",\n",
    "            loss.detach(),\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def validation_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor, List[str]], *args\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        A single validation step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : Tuple[torch.Tensor, torch.Tensor, List[str]]\n",
    "            A batch of (i) MS/MS spectra, (ii) precursor information, (iii)\n",
    "            peptide sequences.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The loss of the validation step.\n",
    "        \"\"\"\n",
    "        # Record the loss.\n",
    "        loss = self.training_step(batch, mode=\"valid\")\n",
    "        if not self.calculate_precision:\n",
    "            return loss\n",
    "\n",
    "        # Calculate and log amino acid and peptide match evaluation metrics from\n",
    "        # the predicted peptides.\n",
    "        peptides_pred, peptides_true = [], batch[2]\n",
    "        for spectrum_preds in self.forward(batch[0], batch[1]):\n",
    "            for _, _, pred in spectrum_preds:\n",
    "                peptides_pred.append(pred)\n",
    "\n",
    "        aa_precision, _, pep_precision = aa_match_metrics(\n",
    "            *aa_match_batch(\n",
    "                peptides_true,\n",
    "                peptides_pred,\n",
    "                self.decoder._peptide_mass.masses,\n",
    "            )\n",
    "        )\n",
    "        log_args = dict(on_step=False, on_epoch=True, sync_dist=True)\n",
    "        self.log(\n",
    "            \"Peptide precision at coverage=1\",\n",
    "            pep_precision,\n",
    "            **log_args,\n",
    "        )\n",
    "        self.log(\n",
    "            \"AA precision at coverage=1\",\n",
    "            aa_precision,\n",
    "            **log_args,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def predict_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], *args\n",
    "    ) -> List[Tuple[np.ndarray, float, float, str, float, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        A single prediction step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "            A batch of (i) MS/MS spectra, (ii) precursor information, (iii)\n",
    "            spectrum identifiers as torch Tensors.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predictions: List[Tuple[np.ndarray, float, float, str, float, np.ndarray]]\n",
    "            Model predictions for the given batch of spectra containing spectrum\n",
    "            ids, precursor information, peptide sequences as well as peptide\n",
    "            and amino acid-level confidence scores.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for (\n",
    "            precursor_charge,\n",
    "            precursor_mz,\n",
    "            spectrum_i,\n",
    "            spectrum_preds,\n",
    "        ) in zip(\n",
    "            batch[1][:, 1].cpu().detach().numpy(),\n",
    "            batch[1][:, 2].cpu().detach().numpy(),\n",
    "            batch[2],\n",
    "            self.forward(batch[0], batch[1]),\n",
    "        ):\n",
    "            for peptide_score, aa_scores, peptide in spectrum_preds:\n",
    "                predictions.append(\n",
    "                    (\n",
    "                        spectrum_i,\n",
    "                        precursor_charge,\n",
    "                        precursor_mz,\n",
    "                        peptide,\n",
    "                        peptide_score,\n",
    "                        aa_scores,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        \"\"\"\n",
    "        Log the training loss at the end of each epoch.\n",
    "        \"\"\"\n",
    "        train_loss = self.trainer.callback_metrics[\"train_CELoss\"].detach()\n",
    "        metrics = {\n",
    "            \"step\": self.trainer.global_step,\n",
    "            \"train\": train_loss.item(),\n",
    "        }\n",
    "        self._history.append(metrics)\n",
    "        self._log_history()\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        \"\"\"\n",
    "        Log the validation metrics at the end of each epoch.\n",
    "        \"\"\"\n",
    "        callback_metrics = self.trainer.callback_metrics\n",
    "        metrics = {\n",
    "            \"step\": self.trainer.global_step,\n",
    "            \"valid\": callback_metrics[\"valid_CELoss\"].detach().item(),\n",
    "        }\n",
    "\n",
    "        if self.calculate_precision:\n",
    "            metrics[\"valid_aa_precision\"] = (\n",
    "                callback_metrics[\"AA precision at coverage=1\"].detach().item()\n",
    "            )\n",
    "            metrics[\"valid_pep_precision\"] = (\n",
    "                callback_metrics[\"Peptide precision at coverage=1\"]\n",
    "                .detach()\n",
    "                .item()\n",
    "            )\n",
    "        self._history.append(metrics)\n",
    "        self._log_history()\n",
    "\n",
    "    def on_predict_batch_end(\n",
    "        self,\n",
    "        outputs: List[Tuple[np.ndarray, List[str], torch.Tensor]],\n",
    "        *args,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Write the predicted peptide sequences and amino acid scores to the\n",
    "        output file.\n",
    "        \"\"\"\n",
    "        if self.out_writer is None:\n",
    "            return\n",
    "        # Triply nested lists: results -> batch -> step -> spectrum.\n",
    "        for (\n",
    "            spectrum_i,\n",
    "            charge,\n",
    "            precursor_mz,\n",
    "            peptide,\n",
    "            peptide_score,\n",
    "            aa_scores,\n",
    "        ) in outputs:\n",
    "            if len(peptide) == 0:\n",
    "                continue\n",
    "            self.out_writer.psms.append(\n",
    "                (\n",
    "                    peptide,\n",
    "                    tuple(spectrum_i),\n",
    "                    peptide_score,\n",
    "                    charge,\n",
    "                    precursor_mz,\n",
    "                    self.peptide_mass_calculator.mass(peptide, charge),\n",
    "                    \",\".join(list(map(\"{:.5f}\".format, aa_scores))),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def _log_history(self) -> None:\n",
    "        \"\"\"\n",
    "        Write log to console, if requested.\n",
    "        \"\"\"\n",
    "        # Log only if all output for the current epoch is recorded.\n",
    "        if len(self._history) == 0:\n",
    "            return\n",
    "        if len(self._history) == 1:\n",
    "            header = \"Step\\tTrain loss\\tValid loss\\t\"\n",
    "            if self.calculate_precision:\n",
    "                header += \"Peptide precision\\tAA precision\"\n",
    "\n",
    "            logger.info(header)\n",
    "        metrics = self._history[-1]\n",
    "        if metrics[\"step\"] % self.n_log == 0:\n",
    "            msg = \"%i\\t%.6f\\t%.6f\"\n",
    "            vals = [\n",
    "                metrics[\"step\"],\n",
    "                metrics.get(\"train\", np.nan),\n",
    "                metrics.get(\"valid\", np.nan),\n",
    "            ]\n",
    "\n",
    "            if self.calculate_precision:\n",
    "                msg += \"\\t%.6f\\t%.6f\"\n",
    "                vals += [\n",
    "                    metrics.get(\"valid_pep_precision\", np.nan),\n",
    "                    metrics.get(\"valid_aa_precision\", np.nan),\n",
    "                ]\n",
    "\n",
    "            logger.info(msg, *vals)\n",
    "            if self.tb_summarywriter is not None:\n",
    "                for descr, key in [\n",
    "                    (\"loss/train_crossentropy_loss\", \"train\"),\n",
    "                    (\"loss/val_crossentropy_loss\", \"valid\"),\n",
    "                    (\"eval/val_pep_precision\", \"valid_pep_precision\"),\n",
    "                    (\"eval/val_aa_precision\", \"valid_aa_precision\"),\n",
    "                ]:\n",
    "                    metric_value = metrics.get(key, np.nan)\n",
    "                    if not np.isnan(metric_value):\n",
    "                        self.tb_summarywriter.add_scalar(\n",
    "                            descr, metric_value, metrics[\"step\"]\n",
    "                        )\n",
    "\n",
    "    def configure_optimizers(\n",
    "        self,\n",
    "    ) -> Tuple[torch.optim.Optimizer, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Initialize the optimizer.\n",
    "\n",
    "        This is used by pytorch-lightning when preparing the model for training.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[torch.optim.Optimizer, Dict[str, Any]]\n",
    "            The initialized Adam optimizer and its learning rate scheduler.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), **self.opt_kwargs)\n",
    "        # Apply learning rate scheduler per step.\n",
    "        lr_scheduler = CosineWarmupScheduler(\n",
    "            optimizer, self.warmup_iters, self.cosine_schedule_period_iters\n",
    "        )\n",
    "        return [optimizer], {\"scheduler\": lr_scheduler, \"interval\": \"step\"}\n",
    "\n",
    "\n",
    "class CosineWarmupScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "    \"\"\"\n",
    "    Learning rate scheduler with linear warm-up followed by cosine shaped decay.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer object.\n",
    "    warmup_iters : int\n",
    "        The number of iterations for the linear warm-up of the learning rate.\n",
    "    cosine_schedule_period_iters : int\n",
    "        The number of iterations for the cosine half period of the learning rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        warmup_iters: int,\n",
    "        cosine_schedule_period_iters: int,\n",
    "    ):\n",
    "        self.warmup_iters = warmup_iters\n",
    "        self.cosine_schedule_period_iters = cosine_schedule_period_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (\n",
    "            1 + np.cos(np.pi * epoch / self.cosine_schedule_period_iters)\n",
    "        )\n",
    "        if epoch <= self.warmup_iters:\n",
    "            lr_factor *= epoch / self.warmup_iters\n",
    "        return lr_factor\n",
    "\n",
    "\n",
    "def _calc_mass_error(\n",
    "    calc_mz: float, obs_mz: float, charge: int, isotope: int = 0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the mass error in ppm between the theoretical m/z and the observed\n",
    "    m/z, optionally accounting for an isotopologue mismatch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    calc_mz : float\n",
    "        The theoretical m/z.\n",
    "    obs_mz : float\n",
    "        The observed m/z.\n",
    "    charge : int\n",
    "        The charge.\n",
    "    isotope : int\n",
    "        Correct for the given number of C13 isotopes (default: 0).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The mass error in ppm.\n",
    "    \"\"\"\n",
    "    return (calc_mz - (obs_mz - isotope * 1.00335 / charge)) / obs_mz * 10**6\n",
    "\n",
    "\n",
    "def _aa_pep_score(\n",
    "    aa_scores: np.ndarray, fits_precursor_mz: bool\n",
    ") -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Calculate amino acid and peptide-level confidence score from the raw amino\n",
    "    acid scores.\n",
    "\n",
    "    The peptide score is the mean of the raw amino acid scores. The amino acid\n",
    "    scores are the mean of the raw amino acid scores and the peptide score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    aa_scores : np.ndarray\n",
    "        Amino acid level confidence scores.\n",
    "    fits_precursor_mz : bool\n",
    "        Flag indicating whether the prediction fits the precursor m/z filter.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    aa_scores : np.ndarray\n",
    "        The amino acid scores.\n",
    "    peptide_score : float\n",
    "        The peptide score.\n",
    "    \"\"\"\n",
    "    peptide_score = np.mean(aa_scores)\n",
    "    aa_scores = (aa_scores + peptide_score) / 2\n",
    "    if not fits_precursor_mz:\n",
    "        peptide_score -= 1\n",
    "    return aa_scores, peptide_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65faf81f",
   "metadata": {},
   "source": [
    "**Model Runner**\n",
    "\n",
    "Training and testing functionality for de novo peptide sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ad8d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import tempfile\n",
    "import uuid\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional, Union\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "from depthcharge.data import AnnotatedSpectrumIndex, SpectrumIndex\n",
    "from lightning.pytorch.strategies import DDPStrategy\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "# Config, MztabWriter, DeNovoDataModule, Spec2Pep \n",
    "logger = logging.getLogger(\"casanovo\")\n",
    "\n",
    "class ModelRunner:\n",
    "    \"\"\"A class to run Casanovo models.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : Config object\n",
    "        The casanovo configuration.\n",
    "    model_filename : str, optional\n",
    "        The model filename is required for eval and de novo modes,\n",
    "        but not for training a model from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Config,\n",
    "        model_filename: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize a ModelRunner\"\"\"\n",
    "        self.config = config\n",
    "        self.model_filename = model_filename\n",
    "\n",
    "        # Initialized later:\n",
    "        self.tmp_dir = None\n",
    "        self.trainer = None\n",
    "        self.model = None\n",
    "        self.loaders = None\n",
    "        self.writer = None\n",
    "\n",
    "        # Configure checkpoints.\n",
    "        if config.save_top_k is not None:\n",
    "            self.callbacks = [\n",
    "                ModelCheckpoint(\n",
    "                    dirpath=config.model_save_folder_path,\n",
    "                    monitor=\"valid_CELoss\",\n",
    "                    mode=\"min\",\n",
    "                    save_top_k=config.save_top_k,\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            self.callbacks = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"Enter the context manager\"\"\"\n",
    "        self.tmp_dir = tempfile.TemporaryDirectory()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        \"\"\"Cleanup on exit\"\"\"\n",
    "        self.tmp_dir.cleanup()\n",
    "        self.tmp_dir = None\n",
    "        if self.writer is not None:\n",
    "            self.writer.save()\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_peak_path: Iterable[str],\n",
    "        valid_peak_path: Iterable[str],\n",
    "    ) -> None:\n",
    "        \"\"\"Train the Casanovo model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_peak_path : iterable of str\n",
    "            The path to the MS data files for training.\n",
    "        valid_peak_path : iterable of str\n",
    "            The path to the MS data files for validation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.initialize_trainer(train=True)\n",
    "        self.initialize_model(train=True)\n",
    "\n",
    "        train_index = self._get_index(train_peak_path, True, \"training\")\n",
    "        valid_index = self._get_index(valid_peak_path, True, \"validation\")\n",
    "        self.initialize_data_module(train_index, valid_index)\n",
    "        self.loaders.setup()\n",
    "\n",
    "        self.trainer.fit(\n",
    "            self.model,\n",
    "            self.loaders.train_dataloader(),\n",
    "            self.loaders.val_dataloader(),\n",
    "        )\n",
    "\n",
    "    def evaluate(self, peak_path: Iterable[str]) -> None:\n",
    "        \"\"\"Evaluate peptide sequence preditions from a trained Casanovo model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        peak_path : iterable of str\n",
    "            The path with MS data files for predicting peptide sequences.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.initialize_trainer(train=False)\n",
    "        self.initialize_model(train=False)\n",
    "\n",
    "        test_index = self._get_index(peak_path, True, \"evaluation\")\n",
    "        self.initialize_data_module(test_index=test_index)\n",
    "        self.loaders.setup(stage=\"test\", annotated=True)\n",
    "\n",
    "        self.trainer.validate(self.model, self.loaders.test_dataloader())\n",
    "\n",
    "    def predict(self, peak_path: Iterable[str], output: str) -> None:\n",
    "        \"\"\"Predict peptide sequences with a trained Casanovo model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        peak_path : iterable of str\n",
    "            The path with the MS data files for predicting peptide sequences.\n",
    "        output : str\n",
    "            Where should the output be saved?\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.writer = MztabWriter(Path(output).with_suffix(\".mztab\"))\n",
    "        self.writer.set_metadata(\n",
    "            self.config,\n",
    "            model=str(self.model_filename),\n",
    "            config_filename=self.config.file,\n",
    "        )\n",
    "\n",
    "        self.initialize_trainer(train=False)\n",
    "        self.initialize_model(train=False)\n",
    "        self.model.out_writer = self.writer\n",
    "\n",
    "        test_index = self._get_index(peak_path, False, \"\")\n",
    "        self.writer.set_ms_run(test_index.ms_files)\n",
    "        self.initialize_data_module(test_index=test_index)\n",
    "        self.loaders.setup(stage=\"test\", annotated=False)\n",
    "        self.trainer.predict(self.model, self.loaders.test_dataloader())\n",
    "\n",
    "    def initialize_trainer(self, train: bool) -> None:\n",
    "        \"\"\"Initialize the lightning Trainer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train : bool\n",
    "            Determines whether to set the trainer up for model training\n",
    "            or evaluation / inference.\n",
    "        \"\"\"\n",
    "        trainer_cfg = dict(\n",
    "            accelerator=self.config.accelerator,\n",
    "            devices=1,\n",
    "            enable_checkpointing=False,\n",
    "        )\n",
    "\n",
    "        if train:\n",
    "            if self.config.devices is None:\n",
    "                devices = \"auto\"\n",
    "            else:\n",
    "                devices = self.config.devices\n",
    "\n",
    "            additional_cfg = dict(\n",
    "                devices=devices,\n",
    "                callbacks=self.callbacks,\n",
    "                enable_checkpointing=self.config.save_top_k is not None,\n",
    "                max_epochs=self.config.max_epochs,\n",
    "                num_sanity_val_steps=self.config.num_sanity_val_steps,\n",
    "                strategy=self._get_strategy(),\n",
    "                val_check_interval=self.config.val_check_interval,\n",
    "                check_val_every_n_epoch=None,\n",
    "            )\n",
    "            trainer_cfg.update(additional_cfg)\n",
    "\n",
    "        self.trainer = pl.Trainer(**trainer_cfg)\n",
    "\n",
    "    def initialize_model(self, train: bool) -> None:\n",
    "        \"\"\"Initialize the Casanovo model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train : bool\n",
    "            Determines whether to set the model up for model training or\n",
    "            evaluation / inference.\n",
    "        \"\"\"\n",
    "        model_params = dict(\n",
    "            dim_model=self.config.dim_model,\n",
    "            n_head=self.config.n_head,\n",
    "            dim_feedforward=self.config.dim_feedforward,\n",
    "            n_layers=self.config.n_layers,\n",
    "            dropout=self.config.dropout,\n",
    "            dim_intensity=self.config.dim_intensity,\n",
    "            max_length=self.config.max_length,\n",
    "            residues=self.config.residues,\n",
    "            max_charge=self.config.max_charge,\n",
    "            precursor_mass_tol=self.config.precursor_mass_tol,\n",
    "            isotope_error_range=self.config.isotope_error_range,\n",
    "            min_peptide_len=self.config.min_peptide_len,\n",
    "            n_beams=self.config.n_beams,\n",
    "            top_match=self.config.top_match,\n",
    "            n_log=self.config.n_log,\n",
    "            tb_summarywriter=self.config.tb_summarywriter,\n",
    "            train_label_smoothing=self.config.train_label_smoothing,\n",
    "            warmup_iters=self.config.warmup_iters,\n",
    "            cosine_schedule_period_iters=self.config.cosine_schedule_period_iters,\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            out_writer=self.writer,\n",
    "            calculate_precision=self.config.calculate_precision,\n",
    "        )\n",
    "\n",
    "        # Reconfigurable non-architecture related parameters for a loaded model.\n",
    "        loaded_model_params = dict(\n",
    "            max_length=self.config.max_length,\n",
    "            precursor_mass_tol=self.config.precursor_mass_tol,\n",
    "            isotope_error_range=self.config.isotope_error_range,\n",
    "            n_beams=self.config.n_beams,\n",
    "            min_peptide_len=self.config.min_peptide_len,\n",
    "            top_match=self.config.top_match,\n",
    "            n_log=self.config.n_log,\n",
    "            tb_summarywriter=self.config.tb_summarywriter,\n",
    "            train_label_smoothing=self.config.train_label_smoothing,\n",
    "            warmup_iters=self.config.warmup_iters,\n",
    "            cosine_schedule_period_iters=self.config.cosine_schedule_period_iters,\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            out_writer=self.writer,\n",
    "            calculate_precision=self.config.calculate_precision,\n",
    "        )\n",
    "\n",
    "        if self.model_filename is None:\n",
    "            # Train a model from scratch if no model file is provided.\n",
    "            if train:\n",
    "                self.model = Spec2Pep(**model_params)\n",
    "                return\n",
    "            # Else we're not training, so a model file must be provided.\n",
    "            else:\n",
    "                logger.error(\"A model file must be provided\")\n",
    "                raise ValueError(\"A model file must be provided\")\n",
    "        # Else a model file is provided (to continue training or for inference).\n",
    "\n",
    "        if not Path(self.model_filename).exists():\n",
    "            logger.error(\n",
    "                \"Could not find the model weights at file %s\",\n",
    "                self.model_filename,\n",
    "            )\n",
    "            raise FileNotFoundError(\"Could not find the model weights file\")\n",
    "\n",
    "        # First try loading model details from the weights file, otherwise use\n",
    "        # the provided configuration.\n",
    "        device = torch.empty(1).device  # Use the default device.\n",
    "        try:\n",
    "            self.model = Spec2Pep.load_from_checkpoint(\n",
    "                self.model_filename, map_location=device, **loaded_model_params\n",
    "            )\n",
    "\n",
    "            architecture_params = set(model_params.keys()) - set(\n",
    "                loaded_model_params.keys()\n",
    "            )\n",
    "            for param in architecture_params:\n",
    "                if model_params[param] != self.model.hparams[param]:\n",
    "                    warnings.warn(\n",
    "                        f\"Mismatching {param} parameter in \"\n",
    "                        f\"model checkpoint ({self.model.hparams[param]}) \"\n",
    "                        f\"vs config file ({model_params[param]}); \"\n",
    "                        \"using the checkpoint.\"\n",
    "                    )\n",
    "        except RuntimeError:\n",
    "            # This only doesn't work if the weights are from an older version\n",
    "            try:\n",
    "                self.model = Spec2Pep.load_from_checkpoint(\n",
    "                    self.model_filename,\n",
    "                    map_location=device,\n",
    "                    **model_params,\n",
    "                )\n",
    "            except RuntimeError:\n",
    "                raise RuntimeError(\n",
    "                    \"Weights file incompatible with the current version of \"\n",
    "                    \"Casanovo.\"\n",
    "                )\n",
    "\n",
    "    def initialize_data_module(\n",
    "        self,\n",
    "        train_index: Optional[AnnotatedSpectrumIndex] = None,\n",
    "        valid_index: Optional[AnnotatedSpectrumIndex] = None,\n",
    "        test_index: Optional[\n",
    "            Union[AnnotatedSpectrumIndex, SpectrumIndex]\n",
    "        ] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the data module\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_index : AnnotatedSpectrumIndex, optional\n",
    "            A spectrum index for model training.\n",
    "        valid_index : AnnotatedSpectrumIndex, optional\n",
    "            A spectrum index for validation.\n",
    "        test_index : AnnotatedSpectrumIndex or SpectrumIndex, optional\n",
    "            A spectrum index for evaluation or inference.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            n_devices = self.trainer.num_devices\n",
    "            train_bs = self.config.train_batch_size // n_devices\n",
    "            eval_bs = self.config.predict_batch_size // n_devices\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"Please use `initialize_trainer()` first.\")\n",
    "\n",
    "        self.loaders = DeNovoDataModule(\n",
    "            train_index=train_index,\n",
    "            valid_index=valid_index,\n",
    "            test_index=test_index,\n",
    "            min_mz=self.config.min_mz,\n",
    "            max_mz=self.config.max_mz,\n",
    "            min_intensity=self.config.min_intensity,\n",
    "            remove_precursor_tol=self.config.remove_precursor_tol,\n",
    "            n_workers=self.config.n_workers,\n",
    "            train_batch_size=train_bs,\n",
    "            eval_batch_size=eval_bs,\n",
    "        )\n",
    "\n",
    "    def _get_index(\n",
    "        self,\n",
    "        peak_path: Iterable[str],\n",
    "        annotated: bool,\n",
    "        msg: str = \"\",\n",
    "    ) -> Union[SpectrumIndex, AnnotatedSpectrumIndex]:\n",
    "        \"\"\"Get the spectrum index.\n",
    "\n",
    "        If the file is a SpectrumIndex, only one is allowed. Otherwise multiple\n",
    "        may be specified.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        peak_path : Iterable[str]\n",
    "            The peak files/directories to check.\n",
    "        annotated : bool\n",
    "            Are the spectra expected to be annotated?\n",
    "        msg : str, optional\n",
    "            A string to insert into the error message.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        SpectrumIndex or AnnotatedSpectrumIndex\n",
    "            The spectrum index for training, evaluation, or inference.\n",
    "        \"\"\"\n",
    "        ext = (\".mgf\", \".h5\", \".hdf5\")\n",
    "        if not annotated:\n",
    "            ext += (\".mzml\", \".mzxml\")\n",
    "\n",
    "        msg = msg.strip()\n",
    "        filenames = _get_peak_filenames(peak_path, ext)\n",
    "        if not filenames:\n",
    "            not_found_err = f\"Cound not find {msg} peak files\"\n",
    "            logger.error(not_found_err + \" from %s\", peak_path)\n",
    "            raise FileNotFoundError(not_found_err)\n",
    "\n",
    "        is_index = any([Path(f).suffix in (\".h5\", \".hdf5\") for f in filenames])\n",
    "        if is_index:\n",
    "            if len(filenames) > 1:\n",
    "                h5_err = f\"Multiple {msg} HDF5 spectrum indexes specified\"\n",
    "                logger.error(h5_err)\n",
    "                raise ValueError(h5_err)\n",
    "\n",
    "            index_fname, filenames = filenames[0], None\n",
    "        else:\n",
    "            index_fname = Path(self.tmp_dir.name) / f\"{uuid.uuid4().hex}.hdf5\"\n",
    "\n",
    "        Index = AnnotatedSpectrumIndex if annotated else SpectrumIndex\n",
    "        valid_charge = np.arange(1, self.config.max_charge + 1)\n",
    "        return Index(index_fname, filenames, valid_charge=valid_charge)\n",
    "\n",
    "    def _get_strategy(self) -> Union[str, DDPStrategy]:\n",
    "        \"\"\"Get the strategy for the Trainer.\n",
    "\n",
    "        The DDP strategy works best when multiple GPUs are used. It can work\n",
    "        for CPU-only, but definitely fails using MPS (the Apple Silicon chip)\n",
    "        due to Gloo.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Union[str, DDPStrategy]\n",
    "            The strategy parameter for the Trainer.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.config.accelerator in (\"cpu\", \"mps\"):\n",
    "            return \"auto\"\n",
    "        elif self.config.devices == 1:\n",
    "            return \"auto\"\n",
    "        elif torch.cuda.device_count() > 1:\n",
    "            return DDPStrategy(find_unused_parameters=False, static_graph=True)\n",
    "        else:\n",
    "            return \"auto\"\n",
    "\n",
    "\n",
    "def _get_peak_filenames(\n",
    "    paths: Iterable[str], supported_ext: Iterable[str]\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Get all matching peak file names from the path pattern.\n",
    "\n",
    "    Performs cross-platform path expansion akin to the Unix shell (glob, expand\n",
    "    user, expand vars).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    paths : Iterable[str]\n",
    "        The path pattern(s).\n",
    "    supported_ext : Iterable[str]\n",
    "        Extensions of supported peak file formats.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        The peak file names matching the path pattern.\n",
    "    \"\"\"\n",
    "    found_files = set()\n",
    "    for path in paths:\n",
    "        path = os.path.expanduser(path)\n",
    "        path = os.path.expandvars(path)\n",
    "        for fname in glob.glob(path, recursive=True):\n",
    "            if Path(fname).suffix.lower() in supported_ext:\n",
    "                found_files.add(fname)\n",
    "\n",
    "    return sorted(list(found_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7877a254",
   "metadata": {},
   "source": [
    "**Start Casanovo**\n",
    "\n",
    "运行casanovo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03366e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import functools\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "warnings.formatwarning = lambda message, category, *args, **kwargs: (\n",
    "    f\"{category.__name__}: {message}\"\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    \".*Consider increasing the value of the `num_workers` argument*\",\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    \".*The PyTorch API of nested tensors is in prototype stage*\",\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    \".*Converting mask without torch.bool dtype to bool*\",\n",
    ")\n",
    "\n",
    "import appdirs\n",
    "import depthcharge\n",
    "import github\n",
    "import lightning\n",
    "import requests\n",
    "import rich_click as click\n",
    "import torch\n",
    "import tqdm\n",
    "from lightning.pytorch import seed_everything\n",
    "\n",
    "logger = logging.getLogger(\"casanovo\")\n",
    "def setup_logging(\n",
    "    output: Optional[str],\n",
    "    verbosity: str,\n",
    ") -> Path:\n",
    "    \"\"\"Set up the logger.\"\"\"\n",
    "    if output is None:\n",
    "        output = f\"casanovo_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "    output = Path(output).expanduser().resolve()\n",
    "    \n",
    "    # Clear any existing handlers\n",
    "    root_logger = logging.getLogger()\n",
    "    if root_logger.handlers:\n",
    "        for handler in root_logger.handlers[:]:\n",
    "            root_logger.removeHandler(handler)\n",
    "            \n",
    "    warnings_logger = logging.getLogger(\"py.warnings\")\n",
    "    if warnings_logger.handlers:\n",
    "        for handler in warnings_logger.handlers[:]:\n",
    "            warnings_logger.removeHandler(handler)\n",
    "\n",
    "    logging_levels = {\n",
    "        \"debug\": logging.DEBUG,\n",
    "        \"info\": logging.INFO,\n",
    "        \"warning\": logging.WARNING,\n",
    "        \"error\": logging.ERROR,\n",
    "    }\n",
    "\n",
    "    # Configure logging\n",
    "    logging.captureWarnings(True)\n",
    "    root_logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    # Create formatters\n",
    "    console_formatter = logging.Formatter(\"{levelname}: {message}\", style=\"{\")\n",
    "    file_formatter = logging.Formatter(\n",
    "        \"{asctime} {levelname} [{name}/{processName}] {module}.{funcName} : \"\n",
    "        \"{message}\",\n",
    "        style=\"{\",\n",
    "    )\n",
    "\n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler(sys.stderr)\n",
    "    console_handler.setLevel(logging_levels[verbosity.lower()])\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "    root_logger.addHandler(console_handler)\n",
    "    \n",
    "    # File handler\n",
    "    file_handler = logging.FileHandler(output.with_suffix(\".log\"))\n",
    "    file_handler.setFormatter(file_formatter) \n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    root_logger.addHandler(file_handler)\n",
    "\n",
    "    # Configure dependency loggers\n",
    "    for logger_name in [\n",
    "        \"depthcharge\",\n",
    "        \"fsspec\", \n",
    "        \"github\",\n",
    "        \"h5py\",\n",
    "        \"numba\", \n",
    "        \"pytorch_lightning\",\n",
    "        \"torch\",\n",
    "        \"urllib3\"\n",
    "    ]:\n",
    "        logging.getLogger(logger_name).setLevel(logging.WARNING)\n",
    "        logging.getLogger(logger_name).propagate = False\n",
    "\n",
    "    return output\n",
    "def setup_logging_ori(\n",
    "    output: Optional[str],\n",
    "    verbosity: str,\n",
    ") -> Path:\n",
    "    \"\"\"Set up the logger.\n",
    "\n",
    "    Logging occurs to the command-line and to the given log file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output : Optional[str]\n",
    "        The provided output file name.\n",
    "    verbosity : str\n",
    "        The logging level to use in the console.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    output : Path\n",
    "        The output file path.\n",
    "    \"\"\"\n",
    "    if output is None:\n",
    "        output = f\"casanovo_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "    output = Path(output).expanduser().resolve()\n",
    "\n",
    "    logging_levels = {\n",
    "        \"debug\": logging.DEBUG,\n",
    "        \"info\": logging.INFO,\n",
    "        \"warning\": logging.WARNING,\n",
    "        \"error\": logging.ERROR,\n",
    "    }\n",
    "\n",
    "    # Configure logging.\n",
    "    logging.captureWarnings(True)\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(logging.DEBUG)\n",
    "    warnings_logger = logging.getLogger(\"py.warnings\")\n",
    "\n",
    "    # Formatters for file vs console:\n",
    "    console_formatter = logging.Formatter(\"{levelname}: {message}\", style=\"{\")\n",
    "    log_formatter = logging.Formatter(\n",
    "        \"{asctime} {levelname} [{name}/{processName}] {module}.{funcName} : \"\n",
    "        \"{message}\",\n",
    "        style=\"{\",\n",
    "    )\n",
    "\n",
    "    console_handler = logging.StreamHandler(sys.stderr)\n",
    "    console_handler.setLevel(logging_levels[verbosity.lower()])\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "    root_logger.addHandler(console_handler)\n",
    "    warnings_logger.addHandler(console_handler)\n",
    "    file_handler = logging.FileHandler(output.with_suffix(\".log\"))\n",
    "    file_handler.setFormatter(log_formatter)\n",
    "    root_logger.addHandler(file_handler)\n",
    "    warnings_logger.addHandler(file_handler)\n",
    "\n",
    "    # Disable dependency non-critical log messages.\n",
    "    logging.getLogger(\"depthcharge\").setLevel(\n",
    "        logging_levels[verbosity.lower()]\n",
    "    )\n",
    "    logging.getLogger(\"fsspec\").setLevel(logging.WARNING)\n",
    "    logging.getLogger(\"github\").setLevel(logging.WARNING)\n",
    "    logging.getLogger(\"h5py\").setLevel(logging.WARNING)\n",
    "    logging.getLogger(\"numba\").setLevel(logging.WARNING)\n",
    "    logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n",
    "    logging.getLogger(\"torch\").setLevel(logging.WARNING)\n",
    "    logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "\n",
    "    return output\n",
    "\n",
    "def setup_model(\n",
    "        model: Optional[str],\n",
    "        config: Optional[str],\n",
    "        output: Optional[Path],\n",
    "        is_train: bool,\n",
    ") -> Config:\n",
    "    config = Config(config)\n",
    "    seed_everything(seed=config[\"random_seed\"], workers=True)\n",
    "    if model is None and not is_train:\n",
    "        print('Error, check model path!')\n",
    "\n",
    "    # Log the active configuration.\n",
    "    logger.info(\"Casanovo version %s\")\n",
    "    logger.debug(\"model = %s\", model)\n",
    "    logger.debug(\"config = %s\", config.file)\n",
    "    logger.debug(\"output = %s\", output)\n",
    "    for key, value in config.items():\n",
    "        logger.debug(\"%s = %s\", str(key), str(value))\n",
    "\n",
    "    return config, model\n",
    "\n",
    "def sequence(\n",
    "    peak_path: Tuple[str],\n",
    "    model: Optional[str],\n",
    "    config: Optional[str],\n",
    "    output: Optional[str],\n",
    "    verbosity=\"info\",\n",
    ") -> None:\n",
    "    \"\"\"De novo sequence peptides from tandem mass spectra.\n",
    "\n",
    "    PEAK_PATH must be one or more mzMl, mzXML, or MGF files from which\n",
    "    to sequence peptides.\n",
    "    \"\"\"\n",
    "    output = setup_logging(output, verbosity)\n",
    "    config, model = setup_model(model, config, output, False)\n",
    "    with ModelRunner(config, model) as runner:\n",
    "        logger.info(\"Sequencing peptides from:\")\n",
    "        for peak_file in peak_path:\n",
    "            logger.info(\"  %s\", peak_file)\n",
    "\n",
    "        runner.predict(peak_path, output)\n",
    "\n",
    "    logger.info(\"DONE!\")\n",
    "\n",
    "def train(\n",
    "        train_peak_path: Tuple[str],\n",
    "        validation_peak_path: Tuple[str],\n",
    "        model: Optional[str],\n",
    "        config: Optional[str],\n",
    "        output: Optional[str], \n",
    "        verbosity=\"info\", #默认info   \n",
    ") -> None:\n",
    "    output = setup_logging(output, verbosity)\n",
    "    config, model = setup_model(model, config, output, True)\n",
    "    with ModelRunner(config, model) as runner: #调用modelrunner.train\n",
    "        logger.info(\"Training a model from:\")\n",
    "        for peak_file in train_peak_path:\n",
    "            logger.info(\"  %s\", peak_file)\n",
    "\n",
    "        logger.info(\"Using the following validation files:\")\n",
    "        for peak_file in validation_peak_path:\n",
    "            logger.info(\"  %s\", peak_file)\n",
    "\n",
    "        runner.train(train_peak_path, validation_peak_path)\n",
    "\n",
    "    logger.info(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_data_path = 'C:/czx/Project/Grade0/denovo_sequencing_immunopeptides/task2_casanovo_notebook/test_dataset.mgf'\n",
    "ori_ne_weight = 'C:/czx/Project/Grade0/denovo_sequencing_immunopeptides/casanovo_ckpt/casanovo_nontryptic.ckpt'\n",
    "finetuned_ne_weight = 'C:/czx/Project/Grade0/denovo_sequencing_immunopeptides/casanovo_ckpt/ori_model_train/casanovo_nontryptic_finetuned_timsdata.ckpt'\n",
    "result_path = 'C:/czx/Project/Grade0/denovo_sequencing_immunopeptides/task1_casanovo_evaluation/tims_data/casanovo_search/test_dataset_result.mztab'\n",
    "config_yaml = 'C:/czx/Project/Grade0/denovo_sequencing_immunopeptides/task2_casanovo_notebook/model/config.yaml'\n",
    "finetune_train_path = 'C:/czx/Project/Grade0/denovo_sequencing_immunopeptides/task2_casanovo_notebook/train_dataset.mgf'\n",
    "finetune_val_path = 'C:/czx/Project/Grade0/denovo_sequencing_immunopeptides/task2_casanovo_notebook/val_dataset.mgf'\n",
    "#sequence(peak_path=(seq_data_path,),model=finetuned_ne_weight,config=config_yaml,output=result_path,verbosity=\"info\")\n",
    "train(train_peak_path=(finetune_train_path,),validation_peak_path=(finetune_val_path,), model = None, config=config_yaml,verbosity=\"info\",output=result_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
